## Useful details

### Focus on overall results

The paper does not go into depth about the architecture of the agent used. The
main focus is on optimizations and the learning algorithms employed. The
specific architecture is therefore omitted from the reproduction -- the results
should hold with any generic RL agent policy compatible with the environments
used. Both the explanation method (as well as StateMask) and the refinement
method (as well as StateMask-R) are based on the black-box assumption. The
performance of explanation and refinement methods should therefore be
independent of the internal network structure of the target agent.

As such, the rubric does not check for exact numbers in the reproduction, but
instead verifies that the general trends and takeaways for each individual
result in the paper are reproduced.

### Architectures

It nevertheless is perhaps useful to know that the authors relied on default
policy architectures from Stable Baselines3. When asked, the authors provided
the following information:

> We utilized the default setting ('MlpPolicy') in stable baselines 3 library
> for the policy network and SAC networks in dense/sparse MuJoCo tasks.
> (https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#policy-networks)

> We utilized an MLP for the policy network in the selfish mining task. The
> hidden sizes are [128, 128, 128, 128].

> We utilized an MLP for the policy network in the cage challenge task. The
> hidden sizes are [64,64,64].

> We utilized the default network structure (see
> https://di-engine-docs.readthedocs.io/en/latest/_modules/ding/model/template/vac.html#VAC)
> for the policy network in the autonomous driving task.

### Clarifying the Fidelity Score and Top-K Critical Steps

The fidelity score pipeline is as follows:

- The **explanation method** for which we wish to measure fidelity score (e.g.,
  StateMask) generates step-level importance scores for the trajectory,
  identifying how critical each step is to the agent's final reward.
- A **sliding window** of size $l = L \times K$ (where $L$ is the total
  trajectory length and $K$ is a fraction) is applied to find the **most
  critical segment** of consecutive steps. The segment with the highest average
  importance score is selected.
- The agent is **fast-forwarded** to the start of the identified critical
  segment, and **random actions** are taken for the duration of the segment
  (i.e., for $l$ steps).
- After the random actions, the agent's **policy is allowed to continue**
  executing normally until the episode ends.
- The change in reward is measured:
  - $R$ is the original episode reward (with no randomization).
  - $R'$ is the reward after randomizing actions in the critical segment.
  - The reward change is computed as $d = |R' - R|$.
- The **maximum possible reward change** ($d_{\text{max}}$) is defined as the
  largest reward value that the environment allows within a single episode,
  based on the environment's specifications.
- The **fidelity score** is calculated using the formula:
  $$
  \text{Fidelity Score} = \log \left( \frac{d}{d_{\text{max}}} \right) - \log \left( \frac{l}{L} \right).
  $$
- A higher fidelity score indicates that the explanation method has identified
  steps that are truly critical to the agent's performance.
- The finishing of the episode is governed by the environment end conditions,
  e.g. in some MuJoCo games, the episode end condition is that either the game
  reaches 1000 steps or the agent is unhealthy.

### Hyperparameter figures

Figures 7 and 8 show slightly different information than what is outlined in the
surrounding text in the paper. Specifically what they are truly plotting is:

- Figure 7 is plotting $\lambda$ for $\lambda \in \{0, 0.1, 0.01, 0.001\}$
- Figure 8 is plotting $p$ for $p \in \{0, 0.25, 0.5, 0.75, 1\}$

The legends for each figure are correct.

## Out of scope for reproduction

Section 3.4 is out of scope for the reproduction of this paper.

The refining results of the Sparse-Walker2d game and the hyper-parameter
sensitivity results of all sparse MuJoCo games are out of scope for the
reproduction of this paper.

The qualitative analysis of the autonomous driving case is out of scope for the
reproduction of this paper.

In "Refining based on Different Explanations.", the paper claims "using the
explanation generated by our mask network, the refining achieves the best
outcome across all applications". This claimed result is not significant however
-- as shown in Table 1, under the "Fix Refine; Vary Explanation Methods", the
refining performance achieved when using the "Ours" explanation method is
comparable to the refining performance achieved when using the "StateMask"
explanation method.

This particular claimed result is therefore ignored when judging a reproduction
attempt.

Experiments directly concerned with the Malware Mutation environment are out of
scope for the reproduction of this paper and can therefore be ignored. Note that
if the same experiment is run for other environments, those parts are still in
scope.

"""
Router Agent â€” å°†ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚è·¯ç”±åˆ°åˆé€‚çš„ Workflow
===============================================================================
è‹±æ–‡ç®€ä»‹ï¼š
    * Convert freeâ€‘form user requests into a structured JSON â€œWorkflow Specâ€
    * Select the bestâ€‘fit workflow from a configurable registry
    * Output example:
        {
            "workflow": "code_implementation",
            "arguments": {
                "paper_url": "https://arxiv.org/abs/..."
            }
        }

ä¸­æ–‡ç®€ä»‹ï¼š
    * è´Ÿè´£æŠŠç”¨æˆ·çš„è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬æ¢æˆç»“æ„åŒ– JSONï¼ˆWorkflowÂ Specï¼‰
    * ä¾æ®å†…éƒ¨æ³¨å†Œè¡¨é€‰æ‹©æœ€åˆé€‚çš„å·¥ä½œæµ
    * è¾“å‡ºç¤ºä¾‹è§ä¸Š
"""
import os, json, logging, httpx, pathlib
from functools import lru_cache
from typing import List, Dict,Any,Optional
from utils.json_tools import safe_json_loads
import yaml

from prompts.router_prompt import ROUTER_PROMPT
from openai import OpenAI, AsyncOpenAI               # pip install openai>=1.0.0
try:
    from anthropic import Anthropic, AsyncAnthropic  # pip install anthropic
except ImportError:
    Anthropic = AsyncAnthropic = None                # æœªå®‰è£…æ—¶å ä½
logger = logging.getLogger(__name__)
DEEPSEEK_ENDPOINT = "https://api.deepseek.com/v1/chat/completions"





def _load_key_config(path: str | os.PathLike) -> Dict[str, Dict[str, str]]:
    """
    ä» YAML æ–‡ä»¶åŠ è½½å„ä¾›åº”å•†çš„ APIâ€‘Key é…ç½®ã€‚

    é¢„æœŸæ ¼å¼ç¤ºä¾‹ï¼ˆ~/.llm_api_keys.yamlï¼‰::
        openai:
          api_key: "sk-..."
          organization: "org_123"
          project: "proj_abc"

        anthropic:
          api_key: "anthropic-key"

        deepseek:
          api_key: "ds-key"
    """
    p = pathlib.Path(path).expanduser()
    if not p.is_file():
        logger.warning("âš ï¸ æœªæ‰¾åˆ° APIâ€‘Key é…ç½®æ–‡ä»¶ï¼š%s", p)
        return {}

    try:
        with p.open("r", encoding="utf-8") as f:
            data: Dict[str, Any] = yaml.safe_load(f) or {}
        if not isinstance(data, dict):
            raise ValueError("YAML é¡¶å±‚åº”ä¸ºæ˜ å°„å‹ (dict)")
        # ä»…ä¿ç•™é”®å€¼éƒ½ä¸º str çš„å­é¡¹ï¼Œé˜²æ­¢æ„å¤–ç»“æ„
        clean: Dict[str, Dict[str, str]] = {}
        for provider, kv in data.items():
            if isinstance(kv, dict):
                clean[provider.lower()] = {str(k): str(v) for k, v in kv.items()}
        return clean
    except Exception as e:
        logger.error("âŒ è§£æ APIâ€‘Key YAML å¤±è´¥ï¼š%s", e)
        return {}


class LLMClient:#è¿™ä¸ªç±»ç›¸å½“äºagent
    """
    é€šç”¨ LLM å®¢æˆ·ç«¯ï¼ˆä» JSON è¯»å– APIâ€‘Keyï¼‰
    -----------------------------------------------------------------
    model_name å‰ç¼€è‡ªåŠ¨åˆ¤å®šä¾›åº”å•†ï¼š
        'gpt-' / 'o3' / 'gpt4o' âœ OpenAI
        'claude' / 'anthropic'  âœ Anthropic
        'deepseek'             âœ DeepSeek
    """

    def __init__(
        self,
        model: str = "chatgpt-o3",
        api_key: str | None = None,
        async_mode: bool = False,
        config_path: str | os.PathLike = "D:\PythonProjects\DeepCode-main\mcp_agent.secrets.yaml",
        **extra,
    ):
        self.model_name = model.lower()
        self.async_mode = async_mode
        self.extra = extra

        # â€”â€” â¶ è¯» JSON é…ç½® â€”â€” #
        cfg = _load_key_config(config_path)
        # å°†å„å®¶ key  / ç»„ç»‡ / project ä¿¡æ¯æå–å‡ºæ¥ï¼Œä¾›ä¸‹é¢ä½¿ç”¨
        openai_cfg   = cfg.get("openai", {})
        anthropic_cfg = cfg.get("anthropic", {})
        deepseek_cfg  = cfg.get("deepseek", {})

        # â€”â€” â· ä¾›åº”å•†åˆ†å‘ â€”â€” #
        if self.model_name.startswith(("gpt", "o3")):
            self.provider = "openai"
            self.client = (AsyncOpenAI if async_mode else OpenAI)(
                api_key      = api_key                          # æ˜ç¡®ä¼ å‚ > JSON > ç¯å¢ƒ
                          or openai_cfg.get("api_key")
                          or os.getenv("OPENAI_API_KEY"),
                organization = openai_cfg.get("organization"),
                project      = openai_cfg.get("project"),
                **{k: v for k, v in extra.items() if k in ("base_url",)},
            )

        elif self.model_name.startswith(("claude", "anthropic")):
            if Anthropic is None:
                raise ImportError("è¯·å…ˆ `pip install anthropic`")
            self.provider = "anthropic"
            self.client = (AsyncAnthropic if async_mode else Anthropic)(
                api_key = api_key
                       or anthropic_cfg.get("api_key")
                       or os.getenv("ANTHROPIC_API_KEY")
            )

        elif self.model_name.startswith("deepseek"):
            self.provider = "deepseek"
            self.api_key = api_key \
                        or deepseek_cfg.get("api_key") \
                        or os.getenv("DEEPSEEK_API_KEY")
            if self.api_key is None:
                raise ValueError("DeepSeek éœ€è¦ api_keyï¼Œè¯·åœ¨ JSON æˆ–ç¯å¢ƒå˜é‡ä¸­é…ç½®")

        else:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ¨¡å‹å‰ç¼€: {model}")

        logger.info("LLMClient ready  provider=%s  model=%s", self.provider, model)

    # ------------------------------------------------------------------ #
    # ç»Ÿä¸€ chat() æ¥å£
    # ------------------------------------------------------------------ #
    def chat(self, messages: List[Dict[str, str]], **kw) -> str:
        if self.provider == "openai":
            return self._chat_openai(messages, **kw)
        if self.provider == "anthropic":
            return self._chat_anthropic(messages, **kw)
        if self.provider == "deepseek":
            return self._chat_deepseek(messages, **kw)
        raise RuntimeError("Unknown provider")

    # ===== OpenAI ===================================================== #
    def _chat_openai(self, messages, **kw) -> str:
        resp = self.client.chat.completions.create(
            model=self.model_name, messages=messages, **kw
        )
        return resp.choices[0].message.content

    # ===== Anthropic ================================================== #
    def _chat_anthropic(self, messages, temperature: float = 0.7, **kw) -> str:
        resp = self.client.messages.create(
            model=self.model_name,
            system=next((m["content"] for m in messages if m["role"] == "system"), None),
            messages=[m for m in messages if m["role"] != "system"],
            temperature=temperature,
            **kw,
        )
        return resp.content[0].text

    # ===== DeepSeek =================================================== #
    def _chat_deepseek(self, messages, temperature: float = 0.7, **kw) -> str:
        payload: Dict[str, Any] = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            **kw,
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        with httpx.Client(timeout=60.0) as client:
            r = client.post(DEEPSEEK_ENDPOINT, headers=headers, json=payload)
            if r.status_code != 200:
                print("DeepSeek 400 Body:", r.text)  # â† æ‰“å°æœåŠ¡ç«¯ä¿¡æ¯
            r.raise_for_status()
            r.raise_for_status()
            data = r.json()
        return data["choices"][0]["message"]["content"]

        return data["choices"][0]["message"]["content"]
class RouterAgent:
    """
    RouterAgent
    -----------
    æŠŠç”¨æˆ·è¾“å…¥ âœ LLM âœ è§£æä¸º JSON âœ è¿”å›æœ€åˆé€‚çš„ Workflow è§„æ ¼

    Parameters
    ----------
    logger : logging.Logger, optional
        é¡¹ç›®ç»Ÿä¸€æ—¥å¿—ï¼›è‹¥ä¸ºç©ºåˆ™è‡ªåŠ¨åˆ›å»ºå­ loggerã€‚
    llm_client : LLMClient | None
        å¯æ³¨å…¥è‡ªå®šä¹‰ LLM å®¢æˆ·ç«¯ï¼›ä¸ºç©ºæ—¶ä½¿ç”¨é»˜è®¤é…ç½®ã€‚
    model : str | None
        æŒ‡å®š LLM åç§°ï¼›ä¸ºç©ºæ—¶ç”± LLMClient å†³å®šã€‚

    Public API
    ----------
    route(user_request: str) -> Dict[str, Any]
        ä¸»å…¥å£ã€‚è¿”å›å½¢å¦‚
        {
            "workflow": "<workflow_name>",
            "arguments": { ... }
        }
    """

    # å¦‚æœä½ çš„ç³»ç»Ÿä¸­ workflow åç§°å¾ˆå›ºå®šï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œåˆ—å‡ºä»¥åšæ ¡éªŒ
    _SUPPORTED_WORKFLOWS = {
        "code_implementation",
        "code_implementation_index",
        "codebase_index",
        "research_to_code",
    }

    def __init__(
        self,
        logger: Optional[logging.Logger] = None,
        model: Optional[str] = None,
    ) -> None:
        self.logger = logger or self._create_default_logger()
        self.llm = LLMClient(model)
        self.logger.info("ğŸ”€ RouterAgent initialized (model=%s)", self.llm)

    # --------------------------------------------------------------------- #
    # Public method
    # --------------------------------------------------------------------- #
    @lru_cache(maxsize=128)
    def route(self, user_request: str) -> Dict[str, Any]:
        """
        Route a freeâ€‘form user request to the best workflow.
        å¯¹åŒä¸€å¥è¯å¼€å¯ LRU ç¼“å­˜ï¼Œå¯é¿å…åå¤è°ƒç”¨ LLMã€‚

        Returns
        -------
        dict
            { "workflow": "...", "arguments": {...} }
        """
        self.logger.info("ğŸ” Routing user request ...")

        # 1. è°ƒç”¨ LLM
        messages = [
            {"role": "system", "content": ROUTER_PROMPT},
            {"role": "user", "content": user_request},
        ]
        llm_raw = self.llm.chat(messages)
        self.logger.debug("LLM raw output: %s", llm_raw)
        print(llm_raw)

        # 2. è§£æ JSONï¼ˆå¸¦å®¹é”™ï¼‰
        try:
            router_spec = safe_json_loads(llm_raw)
        except ValueError as exc:
            self.logger.error("âŒ RouterAgent | æ— æ³•è§£æä¸º JSONï¼š%s", exc)
            raise

        # 3. å­—æ®µæ ¡éªŒ & è¡¥ç¼ºçœ
        workflow_name = router_spec.get("workflow")
        #if not workflow_name:
        #    raise ValueError("RouterAgent è¾“å‡ºç¼ºå°‘ `workflow` å­—æ®µ")
        #if workflow_name not in self._SUPPORTED_WORKFLOWS:
        #    self.logger.warning("âš ï¸  workflow=%s ä¸åœ¨æ”¯æŒåˆ—è¡¨å†…ï¼Œä»ç»§ç»­è¿”å›", workflow_name)

        #router_spec.setdefault("arguments", {})
        #self.logger.info("âœ… RouterAgent | Routed to `%s`", Optional[,"æ–°prompt"])
        return router_spec

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _create_default_logger(self) -> logging.Logger:
        logger = logging.getLogger(f"{__name__}.RouterAgent")
        logger.setLevel(logging.INFO)
        return logger


# --------------------------------------------------------------------------- #
# ç®€æ˜“ CLI / å•å…ƒæµ‹è¯•ï¼ˆå¯é€‰ï¼‰
# --------------------------------------------------------------------------- #
if __name__ == "__main__":  # è°ƒè¯•
    import argparse, textwrap, json
#è£…åœ¨å‚æ•°çš„å®¹å™¨
    parser = argparse.ArgumentParser(
        description="Quick interactive test for RouterAgent",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent(
            """\
            ç¤ºä¾‹ï¼š
              python router_agent.py "è¯·å¸®æˆ‘å¤ç°attention is all you need è®ºæ–‡ä»£ç ï¼Œå¹¶ä¸”å†™ä¸€ä¸ªæ–‡æ¡£å‘Šè¯‰æˆ‘è¯ é‡Šå®ƒçš„åŸç†"
              # è‹¥ä¸ä¼ å‚æ•°ï¼Œåˆ™é»˜è®¤ä½¿ç”¨â€œå¸®æˆ‘å¤ç° ResNet è®ºæ–‡ä»£ç â€
            """
        ),
    )
    # æŠŠä½ç½®å‚æ•°æ”¹ä¸ºå¯é€‰å‚æ•° --query / -qï¼Œè®¾ç½®é»˜è®¤å€¼
    parser.add_argument(
        "-q", "--query",
        type=str,
        default="è¯·å¸®æˆ‘å†™ä¸€ä¸ªåŸºäºèµ·æºå¼•æ“çš„æ¸¸æˆï¼Œè¿™ä¸ªæ¸¸æˆå†…å®¹æ˜¯æœ«ä¸–æ±‚ç”Ÿï¼Œä¸€ä¸ªäººéœ€è¦åœ¨ä¸§å°¸ç—…æ¯’çˆ†å‘çš„åºŸåœŸæ”¶é›†ç‰©èµ„å¹¶æ´»ä¸‹å»ã€‚æ¸¸æˆä¸»è¦ç©æ³•åŒ…æ‹¬ç¬¬ä¸€äººç§°å°„å‡»å’Œæ”¶é›†åˆæˆç‰©èµ„ï¼Œä»¥åŠäººç‰©å‰§æƒ…å‘å±•ã€‚è¯·ä½ å¸®æˆ‘å†™ä¸€ä¸ªå®Œæ•´çš„ä»£ç æ¡†æ¶ï¼Œå¹¶è¡¥å……ç›¸åº”çš„å‰§æƒ…å’Œä¸–ç•Œè§‚",
        help="ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚ï¼ˆç¼ºçœä¸ºï¼šå¸®æˆ‘å¤ç° ResNet è®ºæ–‡ä»£ç ï¼‰",
    )
    args = parser.parse_args()#è§£æ

    router = RouterAgent(model="deepseek-chat")  # deepseek çš„æ¨¡å‹åç¤ºä¾‹
    result = router.route(args.query)#æ·»åŠ å‚æ•°
    print(json.dumps(result, indent=2, ensure_ascii=False))

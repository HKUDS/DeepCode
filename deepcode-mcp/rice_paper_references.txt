References from RICE paper:

1. Todorov et al., 2012 - MuJoCo: A physics engine for model-based control
2. Mnih et al., 2013 - Playing atari with deep reinforcement learning
3. Oh et al., 2016 - Control of memory, active perception, and action in minecraft
4. Cai et al., 2023 - Imitation learning from vague feedback
5. Bar-Zur et al., 2023 - WERLMAN: To tackle whale (transactions), go deep (RL)
6. Vyas et al., 2023 - Automated cyber defence: A review
7. Anderson et al., 2018 - Learning to evade static PE machine learning malware models via reinforcement learning
8. Wang et al., 2023 - Autonomous driving based on approximate safe action
9. Cheng et al., 2023 - StateMask: Explaining deep reinforcement learning through state mask
10. Vinyals et al., 2019 - Grandmaster level in StarCraft II using multi-agent reinforcement learning
11. Agarwal et al., 2022 - Reincarnating reinforcement learning: Reusing prior computation to accelerate progress
12. Ho & Ermon, 2016 - Generative adversarial imitation learning
13. Fu et al., 2018 - Learning robust rewards with adverserial inverse reinforcement learning
14. Cai et al., 2022 - Seeing differently, acting similarly: Heterogeneously observable imitation learning
15. Uchendu et al., 2023 - Jump-start reinforcement learning
16. Guan et al., 2021 - Widening the pipeline in human-guided reinforcement learning with explanation and context-aware data augmentation
17. Van Waveren et al., 2022 - Correct me if i'm wrong: Using non-experts to repair reinforcement learning policies
18. Yu et al., 2023 - AIRS: Explanation for deep reinforcement learning based security applications
19. Agarwal et al., 2020, 2021 - PC-PG: Policy cover directed exploration for provable policy gradient learning
20. Li et al., 2023 - Understanding the complexity gains of single-task RL with a curriculum
21. Chang et al., 2023 - Learning to generate better than your LLM
22. Schulman et al., 2017 - Proximal policy optimization algorithms
23. Puri et al., 2019 - Explain your move: Understanding agent actions using specific and relevant feature attribution
24. Guo et al., 2021 - Edge: Explaining deep reinforcement learning policies
25. Burda et al., 2018 - Exploration by random network distillation
26. Kakade & Langford, 2002 - Approximately optimal approximate reinforcement learning
27. Bellemare et al., 2016 - Unifying count-based exploration and intrinsic motivation
28. Ostrovski et al., 2017 - Count-based exploration with neural density models
29. Ren et al., 2019 - Exploration via hindsight goal generation
30. Ecoffet et al., 2019, 2021 - Go-explore: a new approach for hard-exploration problems / First return, then explore
31. Haarnoja et al., 2018 - Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor
32. Li et al., 2022 - MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning
33. Raff et al., 2017 - Malware detection by eating a whole exe
34. Mazoure et al., 2019 - Leveraging exploration in off-policy algorithms via normalizing flows
35. Raffin et al., 2021 - Stable-baselines3: Reliable reinforcement learning implementations
36. Weng et al., 2022 - Tianshou: A highly modularized deep reinforcement learning library
37. Oh et al., 2018 - Self-imitation learning
38. Sundararajan et al., 2017 - Axiomatic attribution for deep networks
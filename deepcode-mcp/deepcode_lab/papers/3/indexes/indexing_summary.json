{
  "indexing_completion_time": "2025-07-15T13:32:04.025159",
  "total_repositories_processed": 4,
  "output_files": {
    "example_forgetting": "/Users/lizongwei/Desktop/LLM_research/Code-Agent/deepcode-mcp/deepcode_lab/papers/3/indexes/example_forgetting_index.json",
    "mend": "/Users/lizongwei/Desktop/LLM_research/Code-Agent/deepcode-mcp/deepcode_lab/papers/3/indexes/mend_index.json",
    "episodic-lifelong-learning": "/Users/lizongwei/Desktop/LLM_research/Code-Agent/deepcode-mcp/deepcode_lab/papers/3/indexes/episodic-lifelong-learning_index.json",
    "Maximally_Interfered_Retrieval": "/Users/lizongwei/Desktop/LLM_research/Code-Agent/deepcode-mcp/deepcode_lab/papers/3/indexes/Maximally_Interfered_Retrieval_index.json"
  },
  "target_structure": "complete_reproduction_plan:\n  paper_info:\n    title: \"What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement\"\n    core_contribution: \"Method to predict which examples will be forgotten during language model updates\"\n\n  # SECTION 1: Complete File Structure with Detailed Specifications\n  file_structure:\n    forgotten-examples/\n    \u251c\u2500\u2500 src/\n    \u2502   \u251c\u2500\u2500 models/\n    \u2502   \u2502   \u251c\u2500\u2500 encoder.py      # Implements sentence encoder h with MLP\n    \u2502   \u2502   \u251c\u2500\u2500 forecasters/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 base.py     # Abstract base forecaster class\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 frequency.py    # Frequency threshold implementation\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 logit.py        # Logit-change based implementation \n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 representation.py # Representation based implementation\n    \u2502   \u2502   \u2514\u2500\u2500 language_models.py   # Base LM wrapper (BART0/FLAN-T5)\n    \u2502   \u251c\u2500\u2500 training/\n    \u2502   \u2502   \u251c\u2500\u2500 trainer.py      # Main training loops\n    \u2502   \u2502   \u2514\u2500\u2500 losses.py       # Loss functions and metrics\n    \u2502   \u2514\u2500\u2500 utils/\n    \u2502       \u251c\u2500\u2500 data.py         # Dataset handling\n    \u2502       \u2514\u2500\u2500 caching.py      # Logit and representation caching\n    \u251c\u2500\u2500 configs/\n    \u2502   \u2514\u2500\u2500 hyperparams.yaml    # All training parameters\n    \u2514\u2500\u2500 experiments/\n        \u251c\u2500\u2500 single_edit.py      # Single error fix experiments\n        \u2514\u2500\u2500 sequential.py       # Sequential update experiments\n\n  # SECTION 2: Algorithm Implementation Details\n  algorithm_implementations:\n    - algorithm: \"Representation-based Forecasting\"\n      location: \"src/models/forecasters/representation.py\"\n      pseudocode: |\n        class RepresentationForecaster:\n          def forward(x_i, y_i, x_j, y_j):\n            # Encode inputs\n            h_i = encoder(x_i, y_i)  # Shape: [T\u00d7H]\n            h_j = encoder(x_j, y_j)  # Shape: [T\u00d7H]\n            \n            # Compute similarity and add prior\n            sim = torch.matmul(h_j, h_i.transpose())  # [1\u00d71]\n            b_j = compute_frequency_prior(x_j)\n            \n            # Final prediction\n            z_ij = torch.sigmoid(sim + b_j)\n            return z_ij\n\n    - algorithm: \"Logit-based Forecasting\"\n      location: \"src/models/forecasters/logit.py\"\n      pseudocode: |\n        class LogitForecaster:\n          def forward(x_i, y_i, x_j, y_j, f_0, f_i):\n            # Get encodings\n            h_i = encoder(x_i, y_i)\n            h_j = encoder(x_j, y_j)\n            \n            # Compute kernel\n            theta = torch.matmul(h_j, h_i.transpose())\n            \n            # Predict updated logits\n            logit_diff = f_i(x_i) - f_0(x_i)\n            pred_logits = theta * logit_diff + f_0(x_j)\n            return pred_logits\n\n  # SECTION 3: Model Architectures\n  model_specifications:\n    - model: \"Sentence Encoder\"\n      file: \"src/models/encoder.py\"\n      architecture: |\n        Input: Text sequence\n        Base: BART0/FLAN-T5 encoder\n        MLP:\n          Layer 1: Linear(hidden_size, 512)\n          ReLU\n          Layer 2: Linear(512, hidden_size)\n        Output: [T\u00d7H] encoded representation\n\n  # SECTION 4: Training Procedures\n  training_procedures:\n    representation_training:\n      file: \"src/training/trainer.py\"\n      steps:\n        1. \"Sample batch of (x_i,y_i) from D_train_R\"\n        2. \"Sample batch of (x_j,y_j) from D_PT\"\n        3. \"Get ground truth forgetting labels\"\n        4. \"Compute predictions using representation model\"\n        5. \"Optimize BCE loss with positive weight=0.1\"\n      \n      hyperparameters:\n        batch_size: 16\n        learning_rates:\n          encoder_lm: 1e-5\n          encoder_mlp: 1e-4\n        max_steps: 100000\n        pos_neg_ratio: \"8:8 per batch\"\n\n  # SECTION 5: Experiments\n  experiments:\n    - name: \"Single Edit Performance\"\n      reproduces: \"Table 1\"\n      script: \"experiments/single_edit.py\"\n      metrics:\n        - \"Edit Success Rate\"\n        - \"EM Drop Ratio\"\n        - \"F1 Score\"\n      models:\n        - \"BART0-Large\"\n        - \"FLAN-T5-Large\"\n        - \"FLAN-T5-3B\"\n\n  # SECTION 6: Dependencies & Environment\n  environment:\n    python: \">=3.8\"\n    packages:\n      - \"torch>=1.8.0\"\n      - \"transformers\"\n      - \"datasets\"\n      \n  # SECTION 7: Missing Details & Defaults\n  missing_details_solutions:\n    - missing: \"Exact MLP dimensions\"\n      solution: \"Use 512 hidden dim based on common practice\"\n    - missing: \"Early stopping criteria\"\n      solution: \"Stop when validation F1 doesn't improve for 3 epochs\"\n\n  # SECTION 8: Implementation Order\n  implementation_roadmap:\n    week_1:\n      - \"Implement encoder architecture with MLP\"\n      - \"Set up data loading and caching\"\n    week_2:\n      - \"Implement representation-based forecaster\"\n      - \"Build training loop with BCE loss\"\n    week_3:\n      - \"Add logit-based forecaster\"\n      - \"Implement evaluation metrics\"\n    week_4:\n      - \"Run single-edit experiments\"\n      - \"Compare results with paper\"\n\n  # SECTION 9: Validation Checklist\n  validation_checklist:\n    model_architecture:\n      - \"[ ] Encoder produces correct output shapes\"\n      - \"[ ] MLP properly initialized\"\n    training:\n      - \"[ ] Loss decreases during training\"\n      - \"[ ] Batch sampling maintains pos/neg ratio\"\n    results:\n      - \"[ ] F1 scores exceed frequency baseline\"\n      - \"[ ] Performance matches Table 1 within margin\"",
  "code_base_path": "/Users/lizongwei/Desktop/LLM_research/Code-Agent/deepcode-mcp/deepcode_lab/papers/3/code_base",
  "configuration": {
    "config_file_used": null,
    "api_config_file": "mcp_agent.secrets.yaml",
    "pre_filtering_enabled": true,
    "min_confidence_score": 0.3,
    "high_confidence_threshold": 0.7,
    "max_file_size": 1048576,
    "max_content_length": 3000,
    "request_delay": 0.5,
    "supported_extensions_count": 26,
    "skip_directories_count": 13
  }
}
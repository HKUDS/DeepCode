{
  "repo_name": "statemask",
  "total_files": 191,
  "file_summaries": [
    {
      "file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/arguments.py",
      "file_type": "Command-line argument configuration file",
      "main_functions": [
        "argparse.ArgumentParser configuration",
        "Command-line parameter definitions"
      ],
      "key_concepts": [
        "Deep reinforcement learning hyperparameters",
        "Distributed training configuration",
        "Model checkpointing and persistence",
        "GPU/CPU device management",
        "Exploration-exploitation parameters",
        "Optimizer settings"
      ],
      "dependencies": [
        "argparse"
      ],
      "summary": "This file defines command-line arguments for DouZero, a PyTorch-based DouDizhu (Chinese card game) AI training system. It configures hyperparameters for distributed reinforcement learning including device allocation, training parameters, model saving intervals, and optimization settings.",
      "lines_of_code": 56,
      "last_modified": "2025-06-29T22:54:41.449630"
    },
    {
      "file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/utils.py",
      "file_type": "Utility module for reinforcement learning environment and buffer management",
      "main_functions": [
        "create_env",
        "get_buffer",
        "create_buffers"
      ],
      "key_concepts": [
        "reinforcement_learning_buffers",
        "multiprocessing_data_sharing",
        "tensor_batching",
        "card_game_environment",
        "actor_learner_architecture",
        "shared_gpu_memory"
      ],
      "dependencies": [
        "torch",
        "numpy",
        "multiprocessing",
        "logging",
        "douzero.env",
        "env_utils"
      ],
      "summary": "This utility module provides buffer management and environment creation functions for a distributed reinforcement learning system, specifically designed for a card game (DouZero). It handles shared tensor buffers between actor and learner processes, manages data batching from queues, and creates game environments with different player positions.",
      "lines_of_code": 209,
      "last_modified": "2025-06-29T22:54:41.452203"
    },
    {
      "file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/masknet.py",
      "file_type": "PyTorch neural network model definition file",
      "main_functions": [
        "layer_init",
        "LandlordLstmModel",
        "FarmerLstmModel"
      ],
      "key_concepts": [
        "LSTM neural networks",
        "Actor-Critic architecture",
        "Policy and value networks",
        "Categorical probability distributions",
        "Orthogonal weight initialization",
        "Multi-layer perceptrons",
        "Reinforcement learning models"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.nn.functional",
        "torch.distributions.Categorical",
        "numpy"
      ],
      "summary": "This file implements PyTorch models for a card game AI system, specifically defining LSTM-based neural networks for different player roles (Landlord and Farmer). The models use an actor-critic architecture with separate policy and value networks, designed for reinforcement learning in what appears to be a Chinese card game context.",
      "lines_of_code": 144,
      "last_modified": "2025-06-29T22:54:41.450184"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "file_type": "PPO (Proximal Policy Optimization) reinforcement learning implementation with game-theoretic extensions",
      "main_functions": [
        "Config",
        "TrajectoryState",
        "Trajectory",
        "TrainInput",
        "Losses"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO)",
        "Monte Carlo Tree Search (MCTS)",
        "AlphaZero integration",
        "Trajectory collection and processing",
        "Advantage estimation with GAE (lambda=0.95)",
        "Policy gradient methods",
        "Regularization with lasso penalty",
        "Multi-agent game environments"
      ],
      "dependencies": [
        "torch",
        "numpy",
        "open_spiel",
        "pyspiel",
        "mask_net.MLP",
        "open_spiel.python.algorithms.mcts",
        "open_spiel.python.algorithms.alpha_zero"
      ],
      "summary": "This file implements a PPO-based reinforcement learning agent designed for game environments, specifically integrating with OpenSpiel and AlphaZero components. It defines data structures for trajectory collection, training inputs, and loss computation, with support for masked neural networks and regularization techniques for multi-agent strategic game learning.",
      "lines_of_code": 568,
      "last_modified": "2025-06-29T22:54:41.504981"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/Breakthrough/mask_net.py",
      "file_type": "Neural network model definition file implementing an Actor-Critic architecture",
      "main_functions": [
        "MLP.__init__",
        "MLP.forward",
        "MLP.inference",
        "MLP.save_checkpoint",
        "MLP.load_checkpoint"
      ],
      "key_concepts": [
        "Actor-Critic architecture",
        "Multi-layer perceptron",
        "Categorical distribution",
        "Policy gradient methods",
        "Value function approximation",
        "Softmax probability distribution"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.distributions.Categorical",
        "os",
        "numpy",
        "math"
      ],
      "summary": "This file implements an MLP-based Actor-Critic neural network for reinforcement learning, featuring separate actor and critic networks with configurable depth and width. The model supports both GPU training (forward method) and CPU inference, with checkpoint saving/loading functionality for model persistence.",
      "lines_of_code": 62,
      "last_modified": "2025-06-29T22:54:41.504852"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "file_type": "Reinforcement Learning implementation file - PPO (Proximal Policy Optimization) with game-theoretic extensions",
      "main_functions": [
        "Config",
        "TrajectoryState",
        "Trajectory",
        "TrainInput",
        "Losses"
      ],
      "key_concepts": [
        "PPO (Proximal Policy Optimization)",
        "MCTS (Monte Carlo Tree Search)",
        "AlphaZero integration",
        "Trajectory collection and management",
        "GAE (Generalized Advantage Estimation)",
        "Policy gradient methods",
        "Regularization with lasso penalty",
        "Multi-agent game environments"
      ],
      "dependencies": [
        "torch",
        "numpy",
        "open_spiel",
        "pyspiel",
        "mask_net",
        "collections",
        "functools",
        "itertools"
      ],
      "summary": "This file implements a PPO-based reinforcement learning algorithm with game-theoretic extensions, specifically designed for OpenSpiel environments. It includes trajectory management, training data structures, and integration with MCTS and AlphaZero components, featuring custom regularization and mask network support for strategic game playing.",
      "lines_of_code": 568,
      "last_modified": "2025-06-29T22:54:41.506430"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/Connect4/mask_net.py",
      "file_type": "Neural network architecture definition file for reinforcement learning",
      "main_functions": [
        "MLP",
        "ConvBlock",
        "ResBlock",
        "forward",
        "inference",
        "save_checkpoint",
        "load_checkpoint"
      ],
      "key_concepts": [
        "actor-critic architecture",
        "multi-layer perceptron",
        "convolutional neural networks",
        "residual blocks",
        "categorical distribution",
        "policy gradient methods",
        "model checkpointing"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.distributions",
        "torch.nn.functional",
        "numpy",
        "os"
      ],
      "summary": "This file implements neural network architectures for reinforcement learning, featuring an MLP class with actor-critic design that outputs policy distributions and value estimates, along with convolutional building blocks (ConvBlock and ResBlock) for processing structured input data. The code includes model persistence functionality and supports both GPU training and CPU inference modes.",
      "lines_of_code": 145,
      "last_modified": "2025-06-29T22:54:41.506318"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "file_type": "PPO (Proximal Policy Optimization) reinforcement learning implementation with game-theoretic extensions",
      "main_functions": [
        "Config",
        "TrajectoryState",
        "Trajectory",
        "TrainInput",
        "Losses"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO)",
        "Monte Carlo Tree Search (MCTS)",
        "AlphaZero integration",
        "Trajectory collection and processing",
        "Advantage estimation with GAE (\u03bb=0.95)",
        "Policy clipping (clip_param=0.2)",
        "Lasso regularization",
        "Multi-agent game environments"
      ],
      "dependencies": [
        "torch",
        "numpy",
        "open_spiel",
        "pyspiel",
        "mask_net.MLP",
        "open_spiel.python.algorithms.mcts",
        "open_spiel.python.algorithms.alpha_zero"
      ],
      "summary": "This file implements a PPO-based reinforcement learning agent designed for game environments, specifically integrating with OpenSpiel and AlphaZero components. It includes trajectory management, advantage estimation, and appears to be part of a larger system for training agents in multi-player games with mask networks and regularization techniques.",
      "lines_of_code": 568,
      "last_modified": "2025-06-29T22:54:41.513443"
    },
    {
      "file_path": "statemask/extensive_form/perfect_games/TicTacToe/mask_net.py",
      "file_type": "PyTorch neural network module implementing an Actor-Critic reinforcement learning model",
      "main_functions": [
        "MLP.__init__",
        "MLP.forward",
        "MLP.inference",
        "MLP.save_checkpoint",
        "MLP.load_checkpoint"
      ],
      "key_concepts": [
        "Actor-Critic architecture",
        "Multi-layer perceptron",
        "Categorical distribution",
        "Policy gradient methods",
        "Value function approximation",
        "Softmax probability distribution"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.distributions.Categorical",
        "os",
        "numpy",
        "math"
      ],
      "summary": "This file implements a Multi-Layer Perceptron (MLP) class that creates an Actor-Critic neural network architecture for reinforcement learning. The model has separate actor and critic networks with configurable depth and width, where the actor outputs action probabilities and the critic estimates state values, with support for model checkpointing and both GPU training and CPU inference modes.",
      "lines_of_code": 62,
      "last_modified": "2025-06-29T22:54:41.513348"
    },
    {
      "file_path": "statemask/normal_form/YouShallNotPass/src/ppo2_mask.py",
      "file_type": "Reinforcement Learning Algorithm Implementation - PPO2 with Masking/Adversarial Extensions",
      "main_functions": [
        "MyPPO2",
        "__init__"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO2)",
        "Actor-Critic Reinforcement Learning",
        "Multi-agent adversarial training",
        "Policy gradient methods",
        "Value function approximation",
        "Opponent modeling",
        "LSTM policy networks",
        "Gradient clipping",
        "Experience replay"
      ],
      "dependencies": [
        "tensorflow",
        "stable_baselines",
        "gym",
        "numpy",
        "multiprocessing",
        "zoo_utils",
        "agent",
        "value"
      ],
      "summary": "This file implements a custom PPO2 (Proximal Policy Optimization) algorithm with extensions for adversarial multi-agent training, including opponent modeling coefficients and victim retraining capabilities. It extends the standard ActorCriticRLModel from stable_baselines with additional parameters for controlling adversarial training dynamics and opponent policy interactions.",
      "lines_of_code": 972,
      "last_modified": "2025-06-29T22:54:42.007086"
    },
    {
      "file_path": "statemask/normal_form/YouShallNotPass/src/environment.py",
      "file_type": "Environment wrapper and monitoring utility for multi-agent reinforcement learning",
      "main_functions": [
        "Monitor",
        "Multi_Monitor",
        "func"
      ],
      "key_concepts": [
        "VecEnvWrapper",
        "game outcome tracking",
        "win rate calculation",
        "adversarial agent monitoring",
        "running statistics",
        "reward normalization"
      ],
      "dependencies": [
        "numpy",
        "gym",
        "stable_baselines",
        "random"
      ],
      "summary": "This file implements environment wrappers that monitor multi-agent game outcomes and track statistics like win rates, ties, and reward differences. It extends stable-baselines VecEnvWrapper to collect game results and compute performance metrics for adversarial training scenarios.",
      "lines_of_code": 426,
      "last_modified": "2025-06-29T22:54:42.006515"
    },
    {
      "file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "file_type": "Machine learning training script for adversarial multi-agent reinforcement learning",
      "main_functions": [
        "argument parsing with argparse",
        "hyperparameter configuration",
        "environment setup for multi-agent games",
        "PPO2 training with custom masking",
        "reward shaping and scheduling"
      ],
      "key_concepts": [
        "adversarial training",
        "multi-agent reinforcement learning",
        "PPO2 algorithm with masking",
        "reward shaping",
        "coefficient scheduling",
        "victim-adversary dynamics",
        "LSTM and MLP policy networks",
        "environment normalization"
      ],
      "dependencies": [
        "gym",
        "stable_baselines",
        "argparse",
        "custom modules: common, scheduling, shaping_wrappers, environment, logger, ppo2_mask, value"
      ],
      "summary": "This is a training script for adversarial multi-agent reinforcement learning that uses a modified PPO2 algorithm with masking to train agents in competitive environments. The script supports various game environments, configurable network architectures (MLP/LSTM), and sophisticated reward shaping with scheduling mechanisms for victim and adversary loss coefficients.",
      "lines_of_code": 211,
      "last_modified": "2025-06-29T22:54:42.006888"
    },
    {
      "file_path": "statemask/normal_form/Pendulum/ppo_lasso.py",
      "file_type": "Reinforcement Learning Implementation - PPO (Proximal Policy Optimization) with LASSO regularization",
      "main_functions": [
        "PPOMemory",
        "ActorNetwork",
        "CriticNetwork",
        "generate_batches",
        "store_memory",
        "forward"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO)",
        "Actor-Critic architecture",
        "Experience replay buffer",
        "Neural network policy approximation",
        "Categorical probability distributions",
        "Batch processing for training",
        "Model checkpointing"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.optim",
        "torch.distributions.categorical",
        "numpy",
        "os"
      ],
      "summary": "This file implements a PPO reinforcement learning agent with separate actor and critic neural networks. It includes a memory buffer for storing and batching experiences, and provides the core components needed for policy gradient training with value function approximation.",
      "lines_of_code": 163,
      "last_modified": "2025-06-29T22:54:41.790569"
    },
    {
      "file_path": "statemask/normal_form/KickAndDefend/src/ppo2_mask.py",
      "file_type": "Reinforcement Learning Algorithm Implementation",
      "main_functions": [
        "MyPPO2"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO)",
        "Actor-Critic Model",
        "Multi-agent reinforcement learning",
        "Policy gradient methods",
        "Adversarial training",
        "Value function approximation",
        "Experience replay",
        "Gradient clipping"
      ],
      "dependencies": [
        "tensorflow",
        "stable_baselines",
        "gym",
        "numpy",
        "multiprocessing",
        "zoo_utils",
        "agent",
        "value"
      ],
      "summary": "This file implements a custom PPO2 (Proximal Policy Optimization) algorithm that extends the standard ActorCriticRLModel for multi-agent scenarios. It includes specialized features for adversarial training with opponent modeling, victim retraining capabilities, and support for both MLP and LSTM policy networks, designed for competitive multi-agent environments.",
      "lines_of_code": 972,
      "last_modified": "2025-06-29T22:54:41.787752"
    },
    {
      "file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "file_type": "Logging configuration utility for reinforcement learning experiments",
      "main_functions": [
        "make_timestamp",
        "gen_multiline_charts",
        "tb_layout",
        "setup_logger"
      ],
      "key_concepts": [
        "TensorBoard logging",
        "Custom scalar layouts",
        "RL metrics tracking",
        "Episode rewards monitoring",
        "Training metrics visualization",
        "Game outcome statistics",
        "Experiment directory structure"
      ],
      "dependencies": [
        "stable_baselines",
        "tensorboard",
        "tensorflow",
        "datetime",
        "os"
      ],
      "summary": "This file configures logging infrastructure for reinforcement learning experiments by setting up TensorBoard with custom dashboard layouts and integrating with Stable Baselines logger. It creates structured experiment directories with timestamps and defines specific metric categories for tracking episode rewards, game outcomes, training progress, and performance statistics.",
      "lines_of_code": 84,
      "last_modified": "2025-06-29T22:54:41.787439"
    },
    {
      "file_path": "statemask/normal_form/KickAndDefend/src/common.py",
      "file_type": "Configuration and utility module for multi-agent competition environments",
      "main_functions": [
        "get_zoo_path"
      ],
      "key_concepts": [
        "environment configuration",
        "agent parameter mapping",
        "trigger vectors",
        "action vectors",
        "path resolution",
        "multi-agent reinforcement learning"
      ],
      "dependencies": [],
      "summary": "This file serves as a configuration module for multi-agent competition environments, defining environment lists, trigger/action mappings with numerical vectors, and a utility function to resolve file paths for pre-trained agent parameters. It acts as a central registry for different competition scenarios like RunToGoal, YouShallNotPass, KickAndDefend, and Sumo variants across different agent types (Ants/Humans).",
      "lines_of_code": 32,
      "last_modified": "2025-06-29T22:54:41.787006"
    },
    {
      "file_path": "statemask/normal_form/KickAndDefend/src/environment.py",
      "file_type": "Environment wrapper and monitoring utility for multi-agent reinforcement learning",
      "main_functions": [
        "Monitor",
        "Multi_Monitor",
        "func",
        "log_callback",
        "step_wait",
        "reset"
      ],
      "key_concepts": [
        "vectorized environment wrapper",
        "game outcome tracking",
        "win rate calculation",
        "adversarial agent monitoring",
        "running statistics",
        "reward shaping with eta parameter"
      ],
      "dependencies": [
        "numpy",
        "gym",
        "stable_baselines",
        "random",
        "collections.Counter",
        "custom modules (common, agent)"
      ],
      "summary": "This file implements environment wrappers that monitor multi-agent game outcomes and statistics. It tracks win rates, ties, and reward metrics for adversarial training scenarios, providing logging capabilities for reinforcement learning experiments with competing agents.",
      "lines_of_code": 384,
      "last_modified": "2025-06-29T22:54:41.787146"
    },
    {
      "file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "file_type": "Machine learning training script for adversarial multi-agent reinforcement learning",
      "main_functions": [
        "argument parsing with argparse",
        "environment configuration and selection",
        "hyperparameter definition for PPO training",
        "victim and adversarial agent coefficient scheduling",
        "multi-environment setup with vectorization"
      ],
      "key_concepts": [
        "adversarial training",
        "multi-agent reinforcement learning",
        "PPO (Proximal Policy Optimization)",
        "reward shaping",
        "coefficient scheduling",
        "victim-adversary dynamics",
        "LSTM and MLP policy networks",
        "environment vectorization"
      ],
      "dependencies": [
        "gym",
        "stable_baselines",
        "argparse",
        "custom modules: common, scheduling, shaping_wrappers, environment, logger, ppo2_mask, value"
      ],
      "summary": "This is a training script for adversarial multi-agent reinforcement learning that uses a modified PPO algorithm with masking capabilities. It supports training agents in competitive environments with configurable victim and adversarial loss coefficients, reward shaping, and various network architectures (MLP/LSTM) for both victim and adversarial agents.",
      "lines_of_code": 211,
      "last_modified": "2025-06-29T22:54:41.787562"
    },
    {
      "file_path": "statemask/normal_form/Cartpole/ppo_lasso.py",
      "file_type": "Reinforcement Learning Implementation - Proximal Policy Optimization (PPO) with LASSO regularization",
      "main_functions": [
        "PPOMemory",
        "ActorNetwork",
        "CriticNetwork",
        "generate_batches",
        "store_memory",
        "forward"
      ],
      "key_concepts": [
        "Proximal Policy Optimization (PPO)",
        "Actor-Critic architecture",
        "Experience replay buffer",
        "Neural network policy approximation",
        "Value function estimation",
        "Categorical probability distributions",
        "Batch processing for training"
      ],
      "dependencies": [
        "torch",
        "torch.nn",
        "torch.optim",
        "torch.distributions.categorical",
        "numpy",
        "os"
      ],
      "summary": "This file implements a PPO reinforcement learning agent with separate actor and critic neural networks. It includes a memory buffer for storing and batching experiences, and provides the core components for policy gradient-based learning with value function approximation.",
      "lines_of_code": 163,
      "last_modified": "2025-06-29T22:54:41.514893"
    },
    {
      "file_path": "statemask/normal_form/Pong/ppo.py",
      "file_type": "Reinforcement Learning Implementation - PPO (Proximal Policy Optimization) for Atari Pong",
      "main_functions": [
        "CNN",
        "make_env"
      ],
      "key_concepts": [
        "PPO algorithm",
        "Actor-Critic architecture",
        "Convolutional Neural Network",
        "Policy gradient methods",
        "GAE (Generalized Advantage Estimation)",
        "Transfer learning",
        "Regularization (Lasso)",
        "Hyperparameter configuration"
      ],
      "dependencies": [
        "torch",
        "gym",
        "stable_baselines3",
        "numpy",
        "PIL",
        "matplotlib",
        "imageio"
      ],
      "summary": "This file implements a PPO reinforcement learning agent with a CNN-based actor-critic architecture specifically designed for playing Atari Pong. It includes comprehensive hyperparameter configuration, transfer learning capabilities, and infrastructure for training, testing, and checkpointing the model with support for parallel environment processing.",
      "lines_of_code": 344,
      "last_modified": "2025-06-29T22:54:41.794204"
    }
  ],
  "relationships": [
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/arguments.py",
      "target_file_path": "config.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Comprehensive argparse configuration structure",
        "Deep RL hyperparameter organization",
        "Device management (GPU/CPU) configuration",
        "Training parameter definitions",
        "Model persistence settings",
        "Distributed training configuration patterns"
      ],
      "potential_contributions": [
        "Complete argument parsing framework for RICE hyperparameters",
        "Device allocation and management logic",
        "Training configuration structure for PPO and exploration parameters",
        "Model checkpointing and saving interval configurations",
        "Optimizer settings organization"
      ],
      "usage_suggestions": "Adapt the argparse structure to define RICE-specific hyperparameters including StateMask network parameters, PPO enhancement settings, RND exploration coefficients, mixed distribution weights, and environment reset frequencies. Retain the device management and model persistence patterns while replacing DouZero-specific parameters with RICE algorithm configurations."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/arguments.py",
      "target_file_path": "src/utils.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Configuration management patterns",
        "Logging and monitoring setup",
        "Device detection and allocation logic",
        "Parameter validation mechanisms",
        "Training utilities organization"
      ],
      "potential_contributions": [
        "Configuration loading and validation utilities",
        "Device management helper functions",
        "Logging configuration for RICE training",
        "Hyperparameter validation logic",
        "Training state management utilities"
      ],
      "usage_suggestions": "Extract the configuration management, device handling, and utility functions from the arguments file to create helper utilities for RICE. Focus on parameter validation, device allocation, and configuration loading functions that can support the main RICE algorithm orchestration."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/arguments.py",
      "target_file_path": "src/core/ppo_enhanced.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Optimizer configuration patterns",
        "Learning rate scheduling parameters",
        "Training batch size and iteration settings",
        "Model update frequency configurations",
        "Exploration-exploitation parameter definitions"
      ],
      "potential_contributions": [
        "PPO-specific hyperparameter structure",
        "Optimizer configuration for enhanced PPO",
        "Training schedule parameter organization",
        "Exploration bonus coefficient settings"
      ],
      "usage_suggestions": "Reference the optimizer settings, learning rate configurations, and training parameters to inform the PPO enhancement implementation. Use the parameter organization patterns to structure PPO-specific hyperparameters including exploration bonuses and policy update frequencies."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/arguments.py",
      "target_file_path": "examples/demo_rice.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Command-line interface patterns",
        "Parameter parsing for demonstrations",
        "Configuration loading examples",
        "Training setup initialization"
      ],
      "potential_contributions": [
        "CLI interface for RICE demonstrations",
        "Parameter configuration examples",
        "Training initialization patterns",
        "Configuration validation examples"
      ],
      "usage_suggestions": "Use the argument parsing patterns to create a command-line interface for the RICE demonstration script. Adapt the parameter definitions to allow users to configure RICE algorithm parameters, environment settings, and training options through command-line arguments."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/utils.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "buffer_management_patterns",
        "multiprocessing_data_sharing",
        "tensor_batching_utilities",
        "configuration_handling"
      ],
      "potential_contributions": [
        "shared_memory_buffer_implementation",
        "data_batching_from_queues",
        "multiprocess_coordination_utilities",
        "tensor_operations_helpers"
      ],
      "usage_suggestions": "Extract the buffer management and tensor batching utilities from create_buffers() and get_buffer() functions. Adapt the shared memory patterns for RICE's distributed training needs, particularly for managing experience replay buffers and coordinating between exploration and exploitation processes."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/utils.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "environment_creation_patterns",
        "multi_position_environment_handling",
        "environment_initialization_logic"
      ],
      "potential_contributions": [
        "environment_factory_pattern",
        "position_based_environment_configuration",
        "environment_wrapper_concepts"
      ],
      "usage_suggestions": "Reference the create_env() function's pattern for environment instantiation and configuration. Adapt the multi-position handling concept for RICE's need to manage different exploration strategies and state distributions across multiple environment instances."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/utils.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "actor_learner_architecture_patterns",
        "shared_gpu_memory_management",
        "distributed_training_coordination"
      ],
      "potential_contributions": [
        "memory_efficient_buffer_sharing",
        "actor_learner_communication_patterns",
        "gpu_memory_optimization_techniques"
      ],
      "usage_suggestions": "Utilize the shared memory and buffer management concepts for PPO's actor-critic architecture in RICE. The buffer sharing patterns can be adapted for managing policy gradients and value function updates across distributed training processes."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/utils.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "utility",
      "confidence_score": 0.4,
      "helpful_aspects": [
        "buffer_size_configuration",
        "multiprocessing_parameters",
        "memory_management_settings"
      ],
      "potential_contributions": [
        "buffer_configuration_templates",
        "memory_allocation_parameters",
        "distributed_system_configuration"
      ],
      "usage_suggestions": "Extract configuration patterns for buffer sizes, memory allocation, and multiprocessing parameters. These can inform RICE's hyperparameter configuration, particularly for managing exploration buffers and distributed training coordination."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/masknet.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "LSTM-based neural network architecture",
        "Actor-critic model design with separate policy and value networks",
        "PyTorch implementation patterns",
        "Orthogonal weight initialization techniques",
        "Multi-layer perceptron components"
      ],
      "potential_contributions": [
        "Core neural network architecture for StateMask implementation",
        "LSTM components for sequential state processing",
        "Actor-critic framework for policy and value estimation",
        "Weight initialization strategies for stable training"
      ],
      "usage_suggestions": "Adapt the LSTM architecture and actor-critic design as the foundation for the StateMask network. The existing layer_init function and MLP structures can be directly reused, while the LSTM components can be modified to handle state masking operations instead of card game states."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/masknet.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Actor-critic architecture components",
        "Policy network implementation",
        "Value network implementation",
        "Categorical distribution handling for policy outputs"
      ],
      "potential_contributions": [
        "Policy network architecture for PPO implementation",
        "Value network design for advantage estimation",
        "Neural network components for policy optimization",
        "Distribution handling for action selection"
      ],
      "usage_suggestions": "Extract the actor-critic components to enhance the PPO implementation. The policy and value network architectures can be adapted for the RICE algorithm's PPO component, particularly the categorical distribution handling and network initialization patterns."
    },
    {
      "repo_file_path": "statemask/extensive_form/imperfect_games/DouZero/douzero/dmc/masknet.py",
      "target_file_path": "rice/src/core/rnd_exploration.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Neural network architecture patterns",
        "Multi-layer perceptron design",
        "PyTorch model implementation structure",
        "Weight initialization techniques"
      ],
      "potential_contributions": [
        "Network architecture patterns for RND target and predictor networks",
        "MLP components for feature extraction",
        "Initialization strategies for exploration networks"
      ],
      "usage_suggestions": "Use the MLP architecture patterns and initialization techniques as building blocks for the Random Network Distillation implementation. The multi-layer structure and orthogonal initialization can be adapted for both target and predictor networks in RND."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "target_file_path": "src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with trajectory collection",
        "Advantage estimation using GAE (lambda=0.95)",
        "Policy gradient methods and loss computation",
        "Multi-agent game environment support",
        "Regularization techniques with lasso penalty"
      ],
      "potential_contributions": [
        "Core PPO algorithm structure and training loop",
        "Trajectory data structures (TrajectoryState, Trajectory)",
        "Loss computation methods (policy, value, entropy losses)",
        "Configuration management for hyperparameters",
        "Integration patterns for game environments"
      ],
      "usage_suggestions": "Use this file as the primary foundation for src/core/ppo_enhanced.py. Extract the PPO core logic, trajectory handling, and loss computation while adapting the exploration bonus integration points. The existing Config, TrajectoryState, and Losses classes can be directly adapted, with modifications to incorporate RND exploration bonuses into the advantage calculation and loss functions."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "target_file_path": "src/core/mask_network.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Integration with masked neural networks",
        "Game-theoretic state masking concepts",
        "OpenSpiel environment compatibility",
        "Multi-agent strategic game handling"
      ],
      "potential_contributions": [
        "Masking integration patterns with PPO",
        "State representation handling for masked environments",
        "Network architecture considerations for masking",
        "Game state processing methods"
      ],
      "usage_suggestions": "Extract the masking-related components and integration patterns from the PPO implementation. Use the existing masked network handling as reference for implementing StateMask functionality, particularly how masking is applied during policy evaluation and training phases."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "target_file_path": "src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "OpenSpiel game environment integration",
        "Multi-agent environment handling",
        "Game state management and transitions",
        "Environment interface patterns"
      ],
      "potential_contributions": [
        "Environment initialization and configuration patterns",
        "State transition handling methods",
        "Multi-agent coordination interfaces",
        "Game-specific environment adaptations"
      ],
      "usage_suggestions": "Reference the environment interaction patterns and game state management approaches. Use the OpenSpiel integration methods as a template for implementing environment interface and state reset functionality, adapting the multi-agent coordination aspects for the RICE system's environment management needs."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "target_file_path": "config.py",
      "relationship_type": "utility",
      "confidence_score": 0.7,
      "helpful_aspects": [
        "Comprehensive hyperparameter configuration",
        "PPO-specific parameter settings",
        "Training configuration management",
        "Game environment configuration"
      ],
      "potential_contributions": [
        "PPO hyperparameter defaults and ranges",
        "Configuration structure and organization",
        "Parameter validation patterns",
        "Environment-specific configuration options"
      ],
      "usage_suggestions": "Extract the Config class structure and hyperparameter definitions to inform the main config.py file. Use the existing parameter organization and default values as a starting point, adapting the game-specific parameters for the RICE system's requirements while maintaining the PPO configuration structure."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/ppo_gmax.py",
      "target_file_path": "src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Training utilities and helper functions",
        "Data structure management",
        "Logging and monitoring patterns",
        "Mathematical utilities for advantage estimation"
      ],
      "potential_contributions": [
        "Trajectory processing utilities",
        "Mathematical helper functions",
        "Data structure conversion methods",
        "Training monitoring utilities"
      ],
      "usage_suggestions": "Extract utility functions related to trajectory processing, mathematical computations (especially GAE calculation), and data structure management. These can be adapted and included in src/utils.py to support the RICE system's configuration and logging needs."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/mask_net.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Actor-Critic architecture implementation",
        "MLP neural network structure",
        "GPU/CPU inference support",
        "Checkpoint save/load functionality",
        "Categorical distribution for action selection",
        "Value function approximation"
      ],
      "potential_contributions": [
        "Core neural network architecture for StateMask",
        "Policy and value function approximation",
        "Model persistence and loading capabilities",
        "Flexible network configuration (depth/width)",
        "Device-agnostic inference methods"
      ],
      "usage_suggestions": "This file can serve as the foundation for the StateMask network implementation. The existing MLP class can be adapted to handle state masking by modifying the forward pass to incorporate mask information. The actor-critic structure aligns well with PPO requirements, and the checkpoint functionality will be essential for RICE's iterative refinement process."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/mask_net.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "Actor-Critic neural network components",
        "Policy gradient compatible architecture",
        "Value function for advantage estimation",
        "Categorical action distribution",
        "GPU training support"
      ],
      "potential_contributions": [
        "Neural network backbone for PPO agent",
        "Policy and value networks for PPO algorithm",
        "Action probability computation",
        "Value estimation for advantage calculation"
      ],
      "usage_suggestions": "The MLP class can be integrated into the PPO implementation as the policy and value networks. The actor output can be used for PPO's policy updates, while the critic provides value estimates for advantage computation. The existing forward method structure supports the gradient computation needed for PPO updates."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/mask_net.py",
      "target_file_path": "rice/src/core/rnd_exploration.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Neural network architecture patterns",
        "MLP implementation for feature learning",
        "Forward pass and inference methods",
        "Network initialization techniques"
      ],
      "potential_contributions": [
        "Network architecture reference for RND predictor/target networks",
        "MLP structure for state feature extraction",
        "Implementation patterns for dual network setup"
      ],
      "usage_suggestions": "The MLP architecture can serve as a template for implementing RND's predictor and target networks. The network structure and forward pass implementation provide a solid foundation for building the feature extraction networks needed for curiosity-driven exploration bonuses."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Breakthrough/mask_net.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Model checkpoint management",
        "Neural network state persistence",
        "Model loading and saving utilities",
        "Device handling for training/inference"
      ],
      "potential_contributions": [
        "Model persistence for iterative refinement",
        "Checkpoint management across RICE iterations",
        "Network state handling utilities"
      ],
      "usage_suggestions": "The checkpoint functionality can be utilized by the RICE orchestration system to save and load model states between refinement iterations. This supports the iterative nature of RICE where models need to be persisted and restored as the algorithm progresses through different phases."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with trajectory management",
        "GAE (Generalized Advantage Estimation) implementation",
        "Policy gradient methods and loss calculations",
        "Training data structures (TrainInput, Losses)",
        "Configuration management for hyperparameters",
        "Integration patterns with neural networks"
      ],
      "potential_contributions": [
        "Core PPO algorithm implementation",
        "Trajectory collection and processing logic",
        "Loss calculation methods",
        "Training loop structure",
        "Hyperparameter configuration patterns"
      ],
      "usage_suggestions": "Extract the core PPO implementation, trajectory management, and GAE calculation methods. Adapt the training structures and loss calculations for the RICE framework. Remove game-specific components (MCTS, AlphaZero) and add exploration bonus integration points for RND."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "src/core/mask_network.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Mask network integration patterns in PPO",
        "State masking concepts for strategic environments",
        "Neural network architecture considerations",
        "Integration with policy networks"
      ],
      "potential_contributions": [
        "Mask network integration patterns",
        "State representation handling",
        "Network architecture insights",
        "Policy masking implementation approaches"
      ],
      "usage_suggestions": "Extract the mask network integration patterns and adapt them for StateMask implementation. Use the neural network integration approaches as reference for implementing state masking in the RICE context."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "src/rice_refiner.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Algorithm orchestration patterns",
        "Component integration architecture",
        "Training loop management",
        "Multi-component coordination (PPO + MCTS + AlphaZero)"
      ],
      "potential_contributions": [
        "System architecture patterns",
        "Component coordination strategies",
        "Training orchestration approaches",
        "Integration design patterns"
      ],
      "usage_suggestions": "Use the multi-component integration patterns as reference for orchestrating PPO, RND, StateMask, and mixed distribution components in the main RICE algorithm. Adapt the coordination patterns for the RICE-specific component interactions."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "config.py",
      "relationship_type": "utility",
      "confidence_score": 0.8,
      "helpful_aspects": [
        "Comprehensive hyperparameter configuration",
        "PPO-specific parameter settings",
        "Training configuration patterns",
        "Algorithm parameter organization"
      ],
      "potential_contributions": [
        "PPO hyperparameter templates",
        "Configuration structure patterns",
        "Parameter organization approaches",
        "Default value references"
      ],
      "usage_suggestions": "Extract the PPO-related configuration parameters and structure as a template for the RICE configuration. Adapt the parameter organization patterns and add RICE-specific parameters (RND coefficients, exploration bonuses, mask network settings)."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "OpenSpiel environment integration patterns",
        "State management approaches",
        "Environment interface design",
        "Game-specific adaptations"
      ],
      "potential_contributions": [
        "Environment interface patterns",
        "State handling approaches",
        "Integration design insights",
        "Environment abstraction concepts"
      ],
      "usage_suggestions": "Reference the environment integration patterns for designing the environment manager interface. Adapt the state management approaches for RICE's mixed initial state distribution and state reset mechanisms."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/ppo_gmax.py",
      "target_file_path": "src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Logging and monitoring patterns",
        "Utility function organization",
        "Training metrics handling",
        "Configuration management utilities"
      ],
      "potential_contributions": [
        "Logging utility patterns",
        "Metrics collection approaches",
        "Configuration handling utilities",
        "Training monitoring tools"
      ],
      "usage_suggestions": "Extract utility functions for logging, metrics collection, and configuration management. Adapt the monitoring patterns for RICE-specific metrics (exploration bonuses, mask effectiveness, distribution entropy)."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/mask_net.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Actor-critic neural network architecture",
        "Policy distribution output for action selection",
        "Value function estimation capabilities",
        "Model checkpointing and persistence",
        "GPU/CPU inference mode switching",
        "Categorical distribution handling"
      ],
      "potential_contributions": [
        "Core neural network architecture for StateMask implementation",
        "Policy and value network components for PPO integration",
        "Model saving/loading infrastructure",
        "Foundation for state masking neural network"
      ],
      "usage_suggestions": "This file can be directly adapted as the core neural network component for the StateMask implementation. The MLP class provides the exact actor-critic architecture needed for PPO-based reinforcement learning. Modify the input/output dimensions to match the RICE environment requirements, and integrate the policy and value outputs with the PPO algorithm in ppo_enhanced.py."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/mask_net.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "Actor-critic architecture compatibility",
        "Policy gradient method foundations",
        "Categorical distribution for action sampling",
        "Value function for advantage estimation",
        "Neural network forward pass implementation"
      ],
      "potential_contributions": [
        "Neural network backend for PPO algorithm",
        "Policy and value function computation",
        "Action distribution sampling mechanism",
        "Model inference capabilities"
      ],
      "usage_suggestions": "Integrate the MLP neural network as the policy and value function approximator within the PPO algorithm. Use the policy output for action sampling and the value output for advantage estimation. The existing forward pass can be adapted to work with PPO's training loop and exploration bonus integration."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/mask_net.py",
      "target_file_path": "rice/src/core/rnd_exploration.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Convolutional neural network building blocks",
        "ResBlock architecture for feature extraction",
        "Neural network component design patterns",
        "Model architecture flexibility"
      ],
      "potential_contributions": [
        "ConvBlock and ResBlock components for RND target/predictor networks",
        "Neural network architectural patterns",
        "Feature extraction capabilities for state representation"
      ],
      "usage_suggestions": "Utilize the ConvBlock and ResBlock components to build the target and predictor networks for Random Network Distillation. These components can process structured state representations and generate intrinsic reward signals based on prediction error between the networks."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/Connect4/mask_net.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Model checkpointing functionality",
        "Save and load checkpoint methods",
        "Model persistence utilities",
        "Configuration handling patterns"
      ],
      "potential_contributions": [
        "Model saving and loading utilities",
        "Checkpoint management functions",
        "Neural network persistence infrastructure"
      ],
      "usage_suggestions": "Extract the save_checkpoint and load_checkpoint functions to create utility functions for model persistence across the RICE system. These can be used to save trained models, resume training, and manage model versions throughout the reinforcement learning process."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "target_file_path": "src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with trajectory management",
        "Advantage estimation using GAE (\u03bb=0.95)",
        "Policy clipping mechanism (clip_param=0.2)",
        "Loss computation functions for policy and value networks",
        "Configuration management for hyperparameters",
        "Multi-agent game environment integration"
      ],
      "potential_contributions": [
        "Core PPO algorithm implementation",
        "Trajectory collection and processing logic",
        "Advantage estimation calculations",
        "Policy gradient loss functions",
        "Training loop components",
        "Hyperparameter configuration structure"
      ],
      "usage_suggestions": "This file can serve as the foundation for src/core/ppo_enhanced.py. Extract the core PPO components (Config, TrajectoryState, Trajectory, Losses) and adapt them to work with exploration bonuses from RND. The existing advantage estimation and policy clipping mechanisms are directly applicable. Modify the loss computation to incorporate exploration bonuses and integrate with the StateMask network for enhanced exploration."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "target_file_path": "src/rice_refiner.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Training orchestration patterns",
        "Multi-component integration (PPO + MCTS + AlphaZero)",
        "Game environment handling",
        "Training loop structure",
        "Component coordination logic"
      ],
      "potential_contributions": [
        "Training coordination patterns",
        "Multi-algorithm integration approach",
        "Environment interaction patterns",
        "Training state management",
        "Component synchronization logic"
      ],
      "usage_suggestions": "Use the training orchestration patterns and multi-component integration approach as a reference for implementing the main RICE algorithm coordination. The way this file manages PPO training with other components (MCTS, AlphaZero) provides a good template for coordinating PPO, RND, StateMask, and mixed distribution components in the RICE system."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "target_file_path": "config.py",
      "relationship_type": "reference",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "Comprehensive hyperparameter configuration",
        "PPO-specific parameters (learning rates, clip ratios)",
        "Training configuration (batch sizes, epochs)",
        "Regularization parameters (Lasso regularization)",
        "Game environment settings"
      ],
      "potential_contributions": [
        "PPO hyperparameter defaults",
        "Training configuration templates",
        "Regularization parameter settings",
        "Learning rate schedules",
        "Batch size and epoch configurations"
      ],
      "usage_suggestions": "Extract the hyperparameter configuration structure and PPO-specific parameters to establish baseline configurations for the RICE system. The existing parameters for learning rates, clipping ratios, and regularization can serve as starting points for the enhanced PPO implementation with exploration bonuses."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "target_file_path": "src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "OpenSpiel game environment integration",
        "Multi-agent environment handling",
        "State management and transitions",
        "Environment reset and initialization patterns"
      ],
      "potential_contributions": [
        "Environment interface patterns",
        "Multi-agent coordination approaches",
        "State transition handling",
        "Environment reset strategies"
      ],
      "usage_suggestions": "Reference the environment integration patterns for handling game environments in the RICE system. The multi-agent coordination and state management approaches can inform the design of the environment manager, particularly for handling state resets and mixed initial state distributions."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/ppo_gmax.py",
      "target_file_path": "src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Trajectory processing utilities",
        "Mathematical computations for advantage estimation",
        "Training metrics and logging patterns",
        "Configuration validation logic"
      ],
      "potential_contributions": [
        "Trajectory manipulation functions",
        "Mathematical utility functions",
        "Logging and metrics collection",
        "Configuration helper functions"
      ],
      "usage_suggestions": "Extract utility functions for trajectory processing, mathematical computations, and configuration management. These can be adapted to support the RICE system's needs for handling trajectories, computing exploration bonuses, and managing system configuration."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/mask_net.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Actor-Critic architecture implementation",
        "Multi-layer perceptron with configurable depth/width",
        "Categorical distribution for action selection",
        "Model checkpointing functionality",
        "GPU/CPU mode switching",
        "Softmax probability distribution"
      ],
      "potential_contributions": [
        "Complete neural network architecture for StateMask",
        "Ready-to-use Actor-Critic implementation",
        "Model persistence and loading capabilities",
        "Flexible network configuration options"
      ],
      "usage_suggestions": "This file can be directly adapted as the core StateMask neural network implementation. The MLP class provides the exact Actor-Critic architecture needed for RICE's state masking component. Modify the input/output dimensions to match the target environment's state and action spaces, and integrate the forward/inference methods with the RICE algorithm's policy learning loop."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/mask_net.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Actor-Critic neural network foundation",
        "Policy gradient compatible architecture",
        "Value function approximation",
        "Categorical action distribution",
        "Model state management"
      ],
      "potential_contributions": [
        "Neural network backbone for PPO implementation",
        "Actor and critic network definitions",
        "Action probability computation methods",
        "Value estimation capabilities"
      ],
      "usage_suggestions": "Extract the Actor-Critic components from this MLP implementation to serve as the neural network foundation for the PPO algorithm. The actor network can be used for policy updates, while the critic network provides value estimates for advantage computation. Integrate with PPO-specific loss functions and exploration bonuses."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/mask_net.py",
      "target_file_path": "rice/src/core/rnd_exploration.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Multi-layer perceptron architecture",
        "Network initialization patterns",
        "Forward pass implementation",
        "Model configuration flexibility"
      ],
      "potential_contributions": [
        "Network architecture reference for RND target/predictor networks",
        "MLP building blocks for exploration bonus computation",
        "Network initialization and configuration patterns"
      ],
      "usage_suggestions": "Use the MLP architecture as a reference for implementing the Random Network Distillation target and predictor networks. The configurable depth and width parameters can be adapted to create the fixed target network and trainable predictor network needed for intrinsic motivation computation."
    },
    {
      "repo_file_path": "statemask/extensive_form/perfect_games/TicTacToe/mask_net.py",
      "target_file_path": "rice/examples/demo_rice.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Complete working neural network example",
        "Model initialization and usage patterns",
        "Checkpoint saving/loading demonstration",
        "Inference mode switching"
      ],
      "potential_contributions": [
        "Demonstration of neural network usage in RL context",
        "Example of model lifecycle management",
        "Reference implementation for integration patterns"
      ],
      "usage_suggestions": "Use this implementation as a reference example in the RICE demonstration script to show how neural networks are initialized, trained, and used for inference. The checkpoint functionality can demonstrate model persistence across training sessions."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/ppo2_mask.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO2 implementation with actor-critic architecture",
        "Advanced gradient clipping and optimization techniques",
        "Multi-agent training capabilities with opponent modeling",
        "LSTM policy network support for sequential decision making",
        "Experience replay and value function approximation",
        "Adversarial training extensions for robust policy learning"
      ],
      "potential_contributions": [
        "Serve as the foundation for PPO with exploration bonuses",
        "Provide proven multi-agent training framework",
        "Contribute adversarial robustness mechanisms",
        "Supply advanced policy gradient optimization techniques"
      ],
      "usage_suggestions": "This file should be adapted as the core PPO implementation for ppo_enhanced.py. The MyPPO2 class can be extended to incorporate exploration bonuses from RND while maintaining the robust adversarial training capabilities. The opponent modeling coefficients and victim retraining features could be leveraged for the RICE algorithm's adaptive exploration strategy."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/ppo2_mask.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Multi-agent orchestration and coordination logic",
        "Adversarial training dynamics management",
        "Policy interaction and opponent modeling framework",
        "Advanced RL algorithm integration patterns"
      ],
      "potential_contributions": [
        "Provide multi-agent coordination patterns for RICE orchestration",
        "Contribute adversarial training management techniques",
        "Supply policy interaction frameworks for algorithm coordination"
      ],
      "usage_suggestions": "Extract the multi-agent coordination and adversarial training management patterns from MyPPO2 to inform the RICE algorithm orchestration. The opponent modeling and policy interaction mechanisms could be adapted for coordinating between the StateMask, RND exploration, and PPO components in the main RICE workflow."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/ppo2_mask.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Multi-agent environment handling patterns",
        "State management for adversarial scenarios",
        "Policy evaluation and interaction frameworks",
        "Experience collection and management"
      ],
      "potential_contributions": [
        "Inform multi-agent environment interface design",
        "Provide state management patterns for complex scenarios",
        "Contribute experience collection strategies"
      ],
      "usage_suggestions": "Reference the multi-agent environment interaction patterns and state management approaches from the PPO2 implementation to design robust environment interfaces that can handle the complex state distributions and exploration scenarios required by the RICE algorithm."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/ppo2_mask.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "PPO hyperparameter configuration patterns",
        "Multi-agent training parameter management",
        "Adversarial training coefficient settings",
        "Policy network architecture configurations"
      ],
      "potential_contributions": [
        "Provide proven PPO hyperparameter defaults",
        "Contribute multi-agent training configuration patterns",
        "Supply adversarial training parameter templates"
      ],
      "usage_suggestions": "Extract the hyperparameter configuration patterns and default values from the MyPPO2 implementation to establish baseline configurations for the RICE system. The opponent modeling coefficients and adversarial training parameters could inform the exploration bonus weighting and algorithm coordination settings."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/environment.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "VecEnvWrapper implementation pattern",
        "Environment monitoring and statistics tracking",
        "Multi-agent environment handling",
        "Reward normalization techniques",
        "Game outcome tracking infrastructure"
      ],
      "potential_contributions": [
        "Core environment wrapper architecture for RICE",
        "Statistical monitoring for training progress",
        "Multi-agent coordination patterns",
        "Performance metrics collection framework"
      ],
      "usage_suggestions": "Adapt the Monitor and Multi_Monitor classes as base classes for RICE's environment manager. The VecEnvWrapper pattern can be extended to handle state masking and mixed distribution resets. The statistical tracking (win rates, ties) can be modified to track exploration metrics and state coverage statistics relevant to RICE's objectives."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/environment.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.72,
      "helpful_aspects": [
        "Running statistics calculation (func function)",
        "Performance monitoring utilities",
        "Data collection and aggregation patterns",
        "Numerical computation helpers"
      ],
      "potential_contributions": [
        "Statistical utility functions for RICE metrics",
        "Performance tracking infrastructure",
        "Data aggregation methods for exploration statistics"
      ],
      "usage_suggestions": "Extract the statistical computation functions (like the 'func' function) into RICE's utils module. These can be adapted to compute exploration bonuses, state coverage metrics, and other RICE-specific statistics. The running average patterns can be useful for tracking RND prediction errors and PPO performance metrics."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/environment.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Reward normalization techniques",
        "Multi-agent training monitoring",
        "Performance metric collection during training",
        "Environment interaction patterns"
      ],
      "potential_contributions": [
        "Reward processing patterns for exploration bonuses",
        "Training progress monitoring integration",
        "Multi-agent coordination insights"
      ],
      "usage_suggestions": "Reference the reward normalization and monitoring patterns when implementing exploration bonus integration in PPO. The multi-agent monitoring approach can inform how to track and balance exploration bonuses across different agents or training phases in RICE."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/environment.py",
      "target_file_path": "rice/tests/test_environment.py",
      "relationship_type": "utility",
      "confidence_score": 0.58,
      "helpful_aspects": [
        "Environment wrapper testing patterns",
        "Statistical monitoring validation",
        "Multi-agent environment test cases",
        "Performance metric verification methods"
      ],
      "potential_contributions": [
        "Test framework for environment monitoring",
        "Validation patterns for statistical computations",
        "Multi-agent testing methodologies"
      ],
      "usage_suggestions": "Use the monitoring and statistical tracking patterns as reference for creating comprehensive tests for RICE's environment manager. The multi-agent testing approach can be adapted to validate state masking, mixed distribution sampling, and exploration bonus calculations in RICE's testing suite."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "PPO2 algorithm implementation with custom modifications",
        "Multi-agent training loop structure",
        "Hyperparameter configuration patterns",
        "Policy network architecture setup (MLP/LSTM)",
        "Training step orchestration and logging"
      ],
      "potential_contributions": [
        "Core PPO training algorithm as foundation for PPO with exploration bonuses",
        "Multi-agent training experience and patterns",
        "Network architecture configuration methods",
        "Training loop structure and optimization steps"
      ],
      "usage_suggestions": "Extract the core PPO2 implementation and training loop structure as the foundation for ppo_enhanced.py. Adapt the multi-agent training patterns and network architecture setup, then integrate exploration bonus mechanisms (RND) into the existing PPO framework. The hyperparameter configuration and training orchestration patterns can be directly leveraged."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Custom masking implementation in PPO2",
        "State masking concepts and patterns",
        "Integration of masking with policy networks",
        "Masking-aware training procedures"
      ],
      "potential_contributions": [
        "Masking implementation patterns and techniques",
        "Integration methods for masking with neural networks",
        "State manipulation and filtering approaches"
      ],
      "usage_suggestions": "Use the masking implementation patterns and integration techniques as reference for developing the StateMask network. The existing masking logic in the PPO2 context can inform how to implement state masking for initial state distribution manipulation in the RICE framework."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "reference",
      "confidence_score": 0.7,
      "helpful_aspects": [
        "Comprehensive hyperparameter configuration using argparse",
        "Environment-specific parameter settings",
        "Network architecture configuration options",
        "Training parameter organization and defaults"
      ],
      "potential_contributions": [
        "Hyperparameter configuration structure and patterns",
        "Default parameter values for RL training",
        "Configuration organization and validation approaches"
      ],
      "usage_suggestions": "Use the argparse-based configuration structure as a template for rice/config.py. Extract relevant hyperparameters for PPO, network architectures, and training settings. Adapt the parameter organization patterns while adding RICE-specific configurations for exploration bonuses, mixed distributions, and state masking."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Multi-agent environment setup and initialization",
        "Environment normalization and wrapping patterns",
        "Game environment configuration and management",
        "Environment state handling in multi-agent context"
      ],
      "potential_contributions": [
        "Environment setup and initialization patterns",
        "Multi-agent environment handling experience",
        "Environment wrapper and normalization techniques"
      ],
      "usage_suggestions": "Reference the environment setup and initialization patterns for implementing environment_manager.py. Use the multi-agent environment handling experience to design the environment interface, particularly for state reset functionality and environment wrapper patterns that could support mixed initial state distributions."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Reward shaping and coefficient scheduling mechanisms",
        "Training progress tracking and logging",
        "Utility functions for RL training workflows",
        "Configuration validation and processing"
      ],
      "potential_contributions": [
        "Scheduling algorithms for dynamic coefficients",
        "Logging and monitoring utilities for RL training",
        "Reward processing and shaping techniques"
      ],
      "usage_suggestions": "Extract utility functions for logging, configuration processing, and scheduling mechanisms to support rice/src/utils.py. The reward shaping and coefficient scheduling patterns could be adapted for managing exploration bonus coefficients and other dynamic parameters in the RICE system."
    },
    {
      "repo_file_path": "statemask/normal_form/YouShallNotPass/src/mask_train.py",
      "target_file_path": "rice/examples/demo_rice.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Complete training script structure and flow",
        "Integration of all training components",
        "Command-line interface and execution patterns",
        "Training workflow orchestration"
      ],
      "potential_contributions": [
        "Training script structure and execution flow",
        "Component integration patterns",
        "Command-line interface design"
      ],
      "usage_suggestions": "Use the overall script structure and training workflow as a template for creating the demo_rice.py example. Adapt the training loop orchestration and component integration patterns to demonstrate the RICE algorithm, replacing adversarial multi-agent training with RICE's exploration-focused approach."
    },
    {
      "repo_file_path": "statemask/normal_form/Pendulum/ppo_lasso.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with actor-critic architecture",
        "Memory buffer system for experience replay",
        "Neural network policy and value function approximation",
        "Batch processing and training infrastructure",
        "Model checkpointing capabilities"
      ],
      "potential_contributions": [
        "Serve as the foundation for PPO with exploration bonuses",
        "Provide proven actor-critic network architectures",
        "Supply memory management and batch processing logic",
        "Offer training loop structure and optimization patterns"
      ],
      "usage_suggestions": "This file should be directly adapted as the base for ppo_enhanced.py. The existing PPO implementation can be extended by adding exploration bonus computation from RND, integrating with the StateMask network for state filtering, and incorporating mixed distribution sampling. The core PPO mechanics (actor-critic networks, memory buffer, batch generation) are already well-implemented and can be preserved while adding the RICE-specific enhancements."
    },
    {
      "repo_file_path": "statemask/normal_form/Pendulum/ppo_lasso.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Training orchestration patterns from PPO implementation",
        "Model management and checkpointing logic",
        "Experience collection and processing workflows",
        "Neural network training coordination"
      ],
      "potential_contributions": [
        "Provide training loop structure for RICE algorithm coordination",
        "Supply model management patterns for multiple networks",
        "Offer experience processing workflows that can be adapted",
        "Contribute optimization and learning rate scheduling approaches"
      ],
      "usage_suggestions": "Extract the training orchestration patterns, model management logic, and coordination mechanisms from the PPO implementation to inform the design of rice_refiner.py. The way this file manages actor-critic training, memory updates, and model checkpointing can serve as a template for coordinating the multiple components (StateMask, RND, PPO) in the RICE system."
    },
    {
      "repo_file_path": "statemask/normal_form/Pendulum/ppo_lasso.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Neural network architecture patterns from actor/critic networks",
        "Forward pass implementation structure",
        "Network initialization and parameter management",
        "Integration patterns with training loops"
      ],
      "potential_contributions": [
        "Provide neural network design patterns for StateMask implementation",
        "Supply network integration approaches with RL training",
        "Offer parameter management and initialization strategies",
        "Contribute network output processing techniques"
      ],
      "usage_suggestions": "Use the neural network architecture patterns from ActorNetwork and CriticNetwork as reference for designing the StateMask network. The forward pass structure, parameter initialization, and integration with the training process can inform how the StateMask network should be implemented and connected to the overall RICE system."
    },
    {
      "repo_file_path": "statemask/normal_form/Pendulum/ppo_lasso.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Hyperparameter patterns from PPO implementation",
        "Learning rate and optimization parameter examples",
        "Network architecture configuration approaches",
        "Training parameter organization"
      ],
      "potential_contributions": [
        "Provide baseline hyperparameter values for PPO components",
        "Supply configuration patterns for neural networks",
        "Offer training parameter organization strategies",
        "Contribute optimization setting examples"
      ],
      "usage_suggestions": "Extract hyperparameter values and configuration patterns from the PPO implementation to populate config.py. The learning rates, network dimensions, batch sizes, and other training parameters used in this working PPO implementation can serve as starting points for the RICE system configuration, ensuring the enhanced PPO component has proven baseline settings."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/ppo2_mask.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO2 implementation with actor-critic architecture",
        "Multi-agent training capabilities",
        "Policy gradient methods with clipping",
        "Value function approximation",
        "Experience replay mechanisms",
        "Gradient clipping and optimization techniques"
      ],
      "potential_contributions": [
        "Core PPO algorithm foundation for exploration bonus integration",
        "Actor-critic model structure for RND integration",
        "Multi-agent training framework for competitive scenarios",
        "Policy network architectures (MLP and LSTM support)",
        "Training loop and optimization procedures"
      ],
      "usage_suggestions": "Use this PPO2 implementation as the base algorithm and extend it to incorporate RND exploration bonuses. The existing actor-critic structure can be enhanced to include intrinsic motivation from Random Network Distillation. Modify the reward computation to combine extrinsic rewards with exploration bonuses, and adapt the multi-agent capabilities for RICE's competitive training scenarios."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/ppo2_mask.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Multi-agent orchestration capabilities",
        "Adversarial training framework",
        "Opponent modeling features",
        "Victim retraining mechanisms",
        "Training coordination logic"
      ],
      "potential_contributions": [
        "Multi-agent coordination patterns for RICE algorithm",
        "Adversarial training structure for competitive scenarios",
        "Training orchestration and scheduling logic",
        "Agent interaction management"
      ],
      "usage_suggestions": "Extract the multi-agent coordination and adversarial training patterns from this PPO2 implementation to inform the RICE orchestration logic. The opponent modeling and victim retraining mechanisms can be adapted for RICE's iterative refinement process, where agents are trained against increasingly sophisticated opponents."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/ppo2_mask.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Multi-agent environment handling",
        "State management for competitive scenarios",
        "Agent interaction coordination",
        "Environment reset and initialization patterns"
      ],
      "potential_contributions": [
        "Multi-agent environment interface patterns",
        "State synchronization mechanisms",
        "Agent coordination protocols",
        "Environment configuration management"
      ],
      "usage_suggestions": "Reference the multi-agent environment handling patterns from this PPO2 implementation when designing the environment manager. The way this code manages multiple agents, state synchronization, and competitive interactions can inform the environment interface design for RICE's multi-agent training scenarios."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/ppo2_mask.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "PPO hyperparameter configurations",
        "Multi-agent training parameters",
        "Network architecture settings",
        "Training schedule configurations"
      ],
      "potential_contributions": [
        "Proven PPO hyperparameter values",
        "Multi-agent training configuration patterns",
        "Network architecture parameter templates",
        "Optimization setting references"
      ],
      "usage_suggestions": "Extract hyperparameter configurations and training settings from this PPO2 implementation to populate the RICE configuration file. The proven parameter values for PPO training, network architectures, and multi-agent scenarios can serve as starting points for RICE's configuration management."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "TensorBoard logging setup with custom layouts",
        "Experiment directory structure creation",
        "Timestamp generation for experiment tracking",
        "RL-specific metric visualization patterns",
        "Integration with Stable Baselines logging"
      ],
      "potential_contributions": [
        "Complete logging infrastructure for RICE experiments",
        "TensorBoard dashboard configuration for RL metrics",
        "Structured experiment organization and tracking",
        "Ready-to-use logging utilities for training visualization"
      ],
      "usage_suggestions": "Integrate the logging functions directly into utils.py to provide comprehensive experiment tracking for RICE. The TensorBoard layouts can be adapted to track RICE-specific metrics like exploration bonuses, state mask effectiveness, and mixed distribution performance. Use setup_logger() to initialize logging for each RICE training run with proper experiment organization."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "reference",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "Episode reward tracking patterns",
        "Training progress monitoring",
        "Game outcome statistics logging",
        "Custom scalar layout definitions for RL metrics"
      ],
      "potential_contributions": [
        "Logging integration points for main algorithm orchestration",
        "Metric tracking patterns for RICE training phases",
        "TensorBoard visualization of algorithm performance",
        "Structured logging of exploration and refinement metrics"
      ],
      "usage_suggestions": "Reference the logging patterns when implementing RICE orchestration. Use the custom scalar layouts to track RICE-specific metrics like state mask utilization, exploration bonus effectiveness, and mixed distribution convergence. Integrate the logger setup at the beginning of training runs."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "reference",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "RL training metrics visualization",
        "Episode-based logging patterns",
        "Performance statistics tracking",
        "TensorBoard integration with PPO training"
      ],
      "potential_contributions": [
        "Logging infrastructure for enhanced PPO training",
        "Visualization of exploration bonuses and policy updates",
        "Training progress monitoring for PPO with RICE enhancements",
        "Custom metrics for exploration bonus effectiveness"
      ],
      "usage_suggestions": "Integrate logging calls within the enhanced PPO implementation to track exploration bonuses, policy updates, and RICE-specific enhancements. Use the custom scalar layouts to visualize how exploration bonuses affect training dynamics and policy performance."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "target_file_path": "rice/examples/demo_rice.py",
      "relationship_type": "utility",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Experiment setup and logging initialization",
        "TensorBoard dashboard configuration",
        "Structured experiment directory creation",
        "Visualization of training metrics"
      ],
      "potential_contributions": [
        "Complete logging setup for demonstration scripts",
        "Visual feedback during RICE demonstrations",
        "Experiment tracking for demo runs",
        "Professional presentation of results"
      ],
      "usage_suggestions": "Use the logger setup functions in the demonstration script to provide visual feedback and experiment tracking. Initialize TensorBoard logging at the start of demos to show real-time training progress and RICE algorithm effectiveness to users."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/logger.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Logging configuration parameters",
        "TensorBoard layout customization",
        "Experiment naming conventions",
        "Metric tracking specifications"
      ],
      "potential_contributions": [
        "Centralized logging configuration management",
        "Customizable TensorBoard layouts for different experiments",
        "Standardized metric naming and organization",
        "Configurable logging levels and outputs"
      ],
      "usage_suggestions": "Extract logging configuration parameters into the main config file, allowing users to customize TensorBoard layouts, experiment naming, and metric tracking preferences. This enables flexible logging setups for different RICE experiment types."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/common.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Environment configuration patterns",
        "Parameter mapping structures",
        "Path resolution utilities",
        "Multi-agent environment definitions"
      ],
      "potential_contributions": [
        "Template for environment-specific configurations",
        "Path resolution utility functions",
        "Environment registry patterns",
        "Parameter mapping structures for different agent types"
      ],
      "usage_suggestions": "Adapt the environment configuration patterns and parameter mapping structures to define RICE-specific hyperparameters and environment settings. The get_zoo_path function pattern could be modified to resolve paths for RICE model checkpoints and configuration files."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/common.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Path resolution utility function",
        "Environment parameter mapping",
        "Configuration data structures",
        "Multi-environment support patterns"
      ],
      "potential_contributions": [
        "Path resolution utilities for model and config files",
        "Environment configuration helpers",
        "Parameter validation and mapping functions",
        "Logging configuration utilities"
      ],
      "usage_suggestions": "Extract the get_zoo_path function and adapt it for RICE-specific path resolution (model checkpoints, logs, configs). Use the environment mapping patterns to create utilities for managing different experimental configurations and environment setups."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/common.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "reference",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Multi-environment configuration patterns",
        "Environment parameter definitions",
        "Agent type mappings",
        "Competition scenario structures"
      ],
      "potential_contributions": [
        "Environment configuration templates",
        "Multi-agent environment handling patterns",
        "Parameter initialization strategies",
        "Environment registry concepts"
      ],
      "usage_suggestions": "Reference the multi-environment configuration approach when designing the environment interface. The environment lists and parameter mappings could inform how to structure environment selection and configuration for RICE experiments across different domains."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/common.py",
      "target_file_path": "rice/examples/demo_rice.py",
      "relationship_type": "reference",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Environment configuration examples",
        "Multi-agent setup patterns",
        "Parameter specification approaches",
        "Competition scenario definitions"
      ],
      "potential_contributions": [
        "Example environment configurations",
        "Multi-agent demonstration patterns",
        "Parameter setup examples",
        "Environment selection logic"
      ],
      "usage_suggestions": "Use the environment configuration patterns as examples for setting up different experimental scenarios in the RICE demonstration. The multi-agent competition setups could provide templates for showcasing RICE's capabilities across various environments."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/environment.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "vectorized environment wrapper implementation",
        "environment monitoring and statistics tracking",
        "reset and step management for multi-agent scenarios",
        "reward tracking and outcome logging"
      ],
      "potential_contributions": [
        "provide foundation for environment interface management",
        "contribute monitoring capabilities for RICE training",
        "offer vectorized environment handling patterns",
        "supply reward tracking and statistics collection"
      ],
      "usage_suggestions": "Adapt the Monitor and Multi_Monitor classes as base classes for RICE's environment manager. The vectorized wrapper pattern and statistics tracking would be valuable for managing state resets and monitoring exploration progress across multiple environment instances."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/environment.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.72,
      "helpful_aspects": [
        "logging callback implementation",
        "running statistics calculation",
        "win rate and performance metrics tracking",
        "configuration parameter handling (eta parameter)"
      ],
      "potential_contributions": [
        "provide logging utilities for RICE experiments",
        "contribute statistical tracking functions",
        "offer performance monitoring capabilities",
        "supply configuration parameter management patterns"
      ],
      "usage_suggestions": "Extract the logging callback function and statistical tracking utilities to support RICE's configuration and logging needs. The running statistics calculations could be adapted for tracking exploration bonuses and state distribution metrics."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/environment.py",
      "target_file_path": "rice/tests/test_environment.py",
      "relationship_type": "reference",
      "confidence_score": 0.68,
      "helpful_aspects": [
        "environment wrapper testing patterns",
        "multi-agent environment validation",
        "reward tracking verification",
        "vectorized environment testing approaches"
      ],
      "potential_contributions": [
        "provide test patterns for environment management",
        "contribute validation approaches for state reset functionality",
        "offer testing strategies for monitoring systems",
        "supply multi-environment testing frameworks"
      ],
      "usage_suggestions": "Use the environment wrapper testing patterns as reference for creating comprehensive tests for RICE's environment manager. The multi-agent testing approaches would be valuable for validating state reset and exploration tracking functionality."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/environment.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "reward shaping with eta parameter",
        "adversarial training monitoring",
        "performance tracking during training",
        "multi-agent reward handling"
      ],
      "potential_contributions": [
        "provide reward shaping parameter management",
        "contribute training monitoring utilities",
        "offer multi-agent reward processing patterns",
        "supply performance tracking during PPO training"
      ],
      "usage_suggestions": "Leverage the reward shaping patterns and eta parameter handling to enhance PPO's exploration bonus integration. The adversarial training monitoring could inform how to track exploration effectiveness during RICE training."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "PPO algorithm implementation with hyperparameter configuration",
        "Multi-agent training setup and vectorized environments",
        "Policy network architecture handling (MLP/LSTM)",
        "Training loop structure and coefficient scheduling",
        "Reward shaping and loss computation patterns"
      ],
      "potential_contributions": [
        "Core PPO training logic and hyperparameter management",
        "Multi-environment vectorization patterns",
        "Policy network initialization and configuration",
        "Training coefficient scheduling mechanisms",
        "Agent interaction and update procedures"
      ],
      "usage_suggestions": "Extract the PPO training core, hyperparameter management, and multi-agent coordination logic. Adapt the coefficient scheduling for exploration bonuses instead of adversarial training. Remove adversarial-specific components while retaining the robust training infrastructure and policy network handling."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.9,
      "helpful_aspects": [
        "Comprehensive argparse-based hyperparameter configuration",
        "Environment-specific parameter settings",
        "Training hyperparameters for PPO (learning rates, batch sizes, etc.)",
        "Network architecture configuration options",
        "Coefficient scheduling parameters"
      ],
      "potential_contributions": [
        "Complete hyperparameter configuration structure",
        "Environment selection and setup parameters",
        "Training configuration templates",
        "Network architecture options",
        "Scheduling parameter definitions"
      ],
      "usage_suggestions": "Directly adapt the argument parsing structure and hyperparameter definitions. Replace adversarial-specific parameters (victim_coef, adv_coef) with RICE-specific parameters (exploration bonuses, mixed distribution weights, RND parameters). Retain the environment configuration and PPO hyperparameters as they are highly relevant."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Environment configuration and selection logic",
        "Multi-environment vectorization setup",
        "Environment parameter handling",
        "Environment initialization patterns"
      ],
      "potential_contributions": [
        "Environment setup and configuration management",
        "Vectorized environment handling",
        "Environment parameter validation",
        "Environment interface standardization"
      ],
      "usage_suggestions": "Extract the environment configuration, selection, and vectorization logic. Adapt the multi-environment setup for RICE's needs, focusing on state reset management and initial state distribution handling. Remove adversarial environment modifications while retaining the robust environment management infrastructure."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "target_file_path": "rice/examples/demo_rice.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Complete training script structure and flow",
        "Integration of multiple components (environment, training, evaluation)",
        "Command-line interface and execution patterns",
        "Training loop organization"
      ],
      "potential_contributions": [
        "Overall script structure and organization",
        "Component integration patterns",
        "Training execution flow",
        "Command-line interface design"
      ],
      "usage_suggestions": "Use as a reference for structuring the RICE demonstration script. Adapt the main training loop structure, component initialization order, and command-line interface. Replace adversarial training logic with RICE algorithm orchestration while maintaining the clear script organization and execution flow."
    },
    {
      "repo_file_path": "statemask/normal_form/KickAndDefend/src/mask_train.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Configuration validation and processing",
        "Hyperparameter scheduling utilities",
        "Training utilities and helper functions",
        "Logging and monitoring patterns"
      ],
      "potential_contributions": [
        "Configuration processing utilities",
        "Scheduling and coefficient management functions",
        "Training helper utilities",
        "Parameter validation functions"
      ],
      "usage_suggestions": "Extract utility functions for configuration processing, parameter validation, and scheduling mechanisms. Adapt the coefficient scheduling utilities for RICE-specific parameters (exploration bonuses, distribution mixing). Create helper functions for common operations while removing adversarial-specific utilities."
    },
    {
      "repo_file_path": "statemask/normal_form/Cartpole/ppo_lasso.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with actor-critic architecture",
        "Memory buffer system for experience replay",
        "Neural network policy and value function approximation",
        "Batch processing utilities for training",
        "Categorical distribution handling for discrete actions"
      ],
      "potential_contributions": [
        "Serve as the base PPO implementation that can be enhanced with exploration bonuses",
        "Provide the core actor-critic networks that can be extended with RND integration",
        "Supply the memory management system for storing experiences with exploration rewards",
        "Offer the foundational policy gradient training loop"
      ],
      "usage_suggestions": "This file should be adapted as the foundation for ppo_enhanced.py by: 1) Integrating exploration bonus mechanisms from RND, 2) Modifying the reward calculation to include intrinsic motivation, 3) Extending the memory buffer to store additional exploration-related data, 4) Adding hooks for StateMask integration to focus exploration on relevant state regions."
    },
    {
      "repo_file_path": "statemask/normal_form/Cartpole/ppo_lasso.py",
      "target_file_path": "rice/src/core/rnd_exploration.py",
      "relationship_type": "reference",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Neural network architecture patterns for policy networks",
        "Experience storage and batch processing mechanisms",
        "Forward pass implementations that can be adapted for RND networks",
        "Memory management for storing network predictions"
      ],
      "potential_contributions": [
        "Provide neural network design patterns for RND predictor and target networks",
        "Supply batch processing utilities for RND training",
        "Offer memory buffer concepts for storing prediction errors",
        "Contribute network initialization and forward pass structures"
      ],
      "usage_suggestions": "Use the neural network architecture patterns and batch processing utilities from this PPO implementation to design the RND predictor and target networks. The memory buffer concept can be extended to store state observations and prediction errors needed for computing intrinsic rewards."
    },
    {
      "repo_file_path": "statemask/normal_form/Cartpole/ppo_lasso.py",
      "target_file_path": "rice/src/rice_refiner.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "Complete RL agent implementation that can be orchestrated",
        "Training loop structure and experience management",
        "Policy update mechanisms that need coordination",
        "Memory buffer that requires periodic clearing/management"
      ],
      "potential_contributions": [
        "Provide the core RL agent that RICE will orchestrate and refine",
        "Supply training procedures that can be integrated into the refinement process",
        "Offer policy update mechanisms that can be coordinated with exploration strategies",
        "Contribute experience management that supports iterative refinement"
      ],
      "usage_suggestions": "Integrate this PPO agent as a core component within the RICE orchestration system. The rice_refiner.py should manage the PPO agent's training cycles, coordinate with StateMask for focused exploration, and handle the iterative refinement process by resetting the agent's experience buffer and adjusting exploration parameters."
    },
    {
      "repo_file_path": "statemask/normal_form/Cartpole/ppo_lasso.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.45,
      "helpful_aspects": [
        "Neural network utility functions and patterns",
        "Batch processing and data handling utilities",
        "Memory management patterns",
        "Configuration patterns for network hyperparameters"
      ],
      "potential_contributions": [
        "Provide utility functions for neural network operations",
        "Supply batch processing helpers for training data",
        "Contribute memory management utilities",
        "Offer configuration patterns for RL hyperparameters"
      ],
      "usage_suggestions": "Extract reusable utility functions from the PPO implementation such as batch generation, network initialization patterns, and memory management helpers. These can be moved to utils.py to support multiple components in the RICE system while avoiding code duplication."
    },
    {
      "repo_file_path": "statemask/normal_form/Pong/ppo.py",
      "target_file_path": "rice/src/core/ppo_enhanced.py",
      "relationship_type": "direct_match",
      "confidence_score": 0.95,
      "helpful_aspects": [
        "Complete PPO implementation with actor-critic architecture",
        "CNN-based policy and value networks suitable for visual environments",
        "GAE (Generalized Advantage Estimation) implementation",
        "Comprehensive hyperparameter configuration system",
        "Training loop with checkpointing and model persistence",
        "Transfer learning capabilities for model initialization"
      ],
      "potential_contributions": [
        "Serve as the base PPO implementation that can be enhanced with exploration bonuses",
        "Provide the core actor-critic architecture for RICE integration",
        "Supply the training infrastructure and hyperparameter management",
        "Offer the CNN architecture for processing visual state representations"
      ],
      "usage_suggestions": "This file should be adapted as the foundation for ppo_enhanced.py by: 1) Integrating RND exploration bonuses into the reward calculation, 2) Adding hooks for StateMask network integration, 3) Modifying the training loop to incorporate mixed initial state distributions, 4) Extending the hyperparameter configuration to include RICE-specific parameters like exploration bonus weights and mask network settings."
    },
    {
      "repo_file_path": "statemask/normal_form/Pong/ppo.py",
      "target_file_path": "rice/config.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.85,
      "helpful_aspects": [
        "Comprehensive hyperparameter configuration structure",
        "Environment-specific parameter organization",
        "Training configuration with learning rates and batch sizes",
        "Model architecture parameters (CNN layers, hidden dimensions)",
        "Regularization parameters (Lasso regularization)",
        "Transfer learning configuration options"
      ],
      "potential_contributions": [
        "Provide the base hyperparameter structure for RICE configuration",
        "Supply proven parameter values for PPO training",
        "Offer configuration patterns for CNN architectures",
        "Contribute regularization and optimization settings"
      ],
      "usage_suggestions": "Extract the hyperparameter configuration structure and adapt it for RICE by: 1) Adding RICE-specific parameters (exploration bonus weights, mask network architecture, RND network settings), 2) Organizing parameters into logical groups (PPO, RND, StateMask, Environment), 3) Including the proven PPO hyperparameters as defaults, 4) Adding configuration validation and environment-specific parameter sets."
    },
    {
      "repo_file_path": "statemask/normal_form/Pong/ppo.py",
      "target_file_path": "rice/src/environment_manager.py",
      "relationship_type": "partial_match",
      "confidence_score": 0.75,
      "helpful_aspects": [
        "Environment creation and configuration (make_env function)",
        "Atari environment preprocessing and wrapping",
        "Multi-environment parallel processing setup",
        "Environment state management and reset functionality",
        "Frame stacking and observation preprocessing"
      ],
      "potential_contributions": [
        "Provide environment setup and preprocessing utilities",
        "Supply parallel environment management infrastructure",
        "Offer proven Atari environment configuration",
        "Contribute state preprocessing and normalization methods"
      ],
      "usage_suggestions": "Adapt the environment management components by: 1) Extracting the make_env function and environment preprocessing logic, 2) Adding support for mixed initial state distribution by modifying reset functionality, 3) Integrating state masking capabilities for StateMask network training, 4) Extending environment interface to support RICE-specific state sampling and reset strategies."
    },
    {
      "repo_file_path": "statemask/normal_form/Pong/ppo.py",
      "target_file_path": "rice/src/core/mask_network.py",
      "relationship_type": "reference",
      "confidence_score": 0.65,
      "helpful_aspects": [
        "CNN architecture design patterns for visual state processing",
        "Network initialization and parameter management",
        "Feature extraction layers suitable for Atari environments",
        "Regularization techniques (Lasso) for network training"
      ],
      "potential_contributions": [
        "Provide CNN architecture patterns for StateMask network design",
        "Supply feature extraction layer configurations",
        "Offer regularization techniques for mask network training",
        "Contribute network initialization strategies"
      ],
      "usage_suggestions": "Use the CNN architecture as a reference for designing the StateMask network by: 1) Adapting the convolutional layers for state representation learning, 2) Modifying the output layer to produce binary masks instead of policy/value outputs, 3) Incorporating similar regularization techniques for mask sparsity, 4) Using comparable network initialization and parameter management strategies."
    },
    {
      "repo_file_path": "statemask/normal_form/Pong/ppo.py",
      "target_file_path": "rice/src/utils.py",
      "relationship_type": "utility",
      "confidence_score": 0.55,
      "helpful_aspects": [
        "Model checkpointing and persistence utilities",
        "Training progress tracking and logging",
        "Configuration management and validation",
        "Performance monitoring and metrics collection"
      ],
      "potential_contributions": [
        "Provide model saving and loading utilities",
        "Supply training progress monitoring infrastructure",
        "Offer configuration validation and management tools",
        "Contribute logging and metrics collection utilities"
      ],
      "usage_suggestions": "Extract utility functions for RICE system support by: 1) Adapting model checkpointing for multiple networks (PPO, RND, StateMask), 2) Creating configuration validation utilities for RICE-specific parameters, 3) Implementing logging systems for exploration bonus tracking and mask network performance, 4) Adding utilities for managing mixed initial state distributions and environment state sampling."
    }
  ],
  "analysis_metadata": {
    "analysis_date": "2025-06-29T23:12:35.244767",
    "target_structure_analyzed": "rice/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 mask_network.py           # StateMask implementation\n\u2502   \u2502   \u251c\u2500\u2500 mixed_distribution.py     # Mixed initial state di...",
    "total_relationships_found": 88,
    "high_confidence_relationships": 43,
    "analyzer_version": "1.3.0",
    "pre_filtering_enabled": true,
    "files_before_filtering": 191,
    "files_after_filtering": 20,
    "filtering_efficiency": 89.53,
    "config_file_used": null,
    "min_confidence_score": 0.3,
    "high_confidence_threshold": 0.7,
    "concurrent_analysis_used": false,
    "content_caching_enabled": false,
    "cache_hits": 0
  }
}